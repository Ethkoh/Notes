a:=b is assignment
a=b is truth assertion

alpha is learning rate

gradient descent algorithn
-use simultaneous update
-when approaching local min, gradient descent automatically takes smaller steps even at same learning rate.

vector
nx1 matrix
usually 1-indexed (eg y1,y2,..) instead of 0-indexed (eg y0,t1,...)

no need feature scaling if using normal equation method compared to gradient descent. magnitude of feature values insignificant compared to computational cost

gradient descent vs normal equation
if number of features is small, use normal equation

applying linear regression to classification problem not a good idea:
-if have extreme values, will cause bad classification
-even if all values are 0 or 1, the hypothesis output can end up being not 0 or 1.

logistic regression is a classification algorithm even though name is regression.
will restrict hypothesis output to between 0 and 1

logistic function=sigmoid function
asymptote at 0 and 1.

dont have to plot training set to plot decision boundary.hypothesis function g(z) is not dependent on training set
decision boundary is created by the hypothesis function

for discrete classification, ouput of hypothesis >=0.5 if y=1 and vice versa.
predict "y=1" if input (z) of logisitic function  z>=0 for its output >=0.5.


cost function of linear regression: sum of squared diff

