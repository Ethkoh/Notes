a:=b is assignment
a=b is truth assertion

alpha is learning rate

gradient descent algorithn
-use simultaneous update
-when approaching local min, gradient descent automatically takes smaller steps even at same learning rate.

vector
nx1 matrix
usually 1-indexed (eg y1,y2,..) instead of 0-indexed (eg y0,t1,...)

no need feature scaling if using normal equation method compared to gradient descent. magnitude of feature values insignificant compared to computational cost

gradient descent vs normal equation
if number of features is small, use normal equation

applying linear regression to classification problem not a good idea:
-if have extreme values, will cause bad classification
-even if all values are 0 or 1, the hypothesis output can end up being not 0 or 1.

logistic regression is a classification algorithm even though name is regression.
will restrict hypothesis output to between 0 and 1

logistic function=sigmoid function
asymptote at 0 and 1.

dont have to plot training set to plot decision boundary.hypothesis function g(z) is not dependent on training set
decision boundary is created by the hypothesis function

for discrete classification, ouput of hypothesis >=0.5 if y=1 and vice versa.
predict "y=1" if input (z) of logisitic function  z>=0 for its output >=0.5.

cost function of linear regression: sum of squared diff
problem of using the same cost function is if were to use sigmoid/logistic function, the cost function can turn out non-convex. there is possibility global min cannot be found cause too many local minimmum.
hence must use the new type of cost function for logistic regression to guarantee that J(θ) is convex for logistic regression.
J(θ)=1/m∑i=1mCost(hθ(x(i)),y(i))
Cost(hθ(x),y)=−log(hθ(x))if y = 1
Cost(hθ(x),y)=−log(1−hθ(x))if y = 0

Cost(hθ(x),y)=0 if hθ(x)=y
Cost(hθ(x),y)→∞ if y=0andhθ(x)→1
Cost(hθ(x),y)→∞ if y=1andhθ(x)→0
summary meant: 
If our correct answer 'y' is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.
If our correct answer 'y' is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.
to find the theta that minimizes the cost function for logisitic regression, use gradient descent.
however, not that the gradient descent is not the same as linear regression even though the update algorithm looks exactly same. because the hypothesis function hθ(x) is not the same. previously is θ^T(x). Now is 1/(1+e^θ^T(x)).

feature scaling help gradient descent converge much faster for linear regression
same can be applied for logistic regression

optimization algorithm:
-gradient descent
-conjugate gradient (adv numerical computing topics)
-BFGS(adv numerical computing topics)
-L-BFGS(adv numerical computing topics)
pros: no need pick learning rate. faster than gradient descent. 
cons: more complex
use algorithms for these provided by Octave already. they are better at optimizing theta.
