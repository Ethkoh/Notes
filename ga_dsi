--------------------------------------------PREWORK-----------------------------------------------------
feature engineering: Create columns derived from our data

What does a compelling data-driven presentation look like?
-Summarizing your findings.
-Labeling all plots and visualizations.
-Restating your hypothesis and initial assumptions.
-Describing your data and process.
-Explaining your model‚Äôs strengths and limitations.
-Providing an appropriate degree of disclosure for your audience (especially when dealing with proprietary data or sensitive user information).

range(x): 0 to x, not inclusive of x
range(a,b,c): a=start,b=stop,c=step

One way of vectorizing your data is called label encoding ‚Äî assigning numerical values to each attribute.
Another common technique for vectorizing categorical data is called one-hot encoding (sometimes abbreviated as ‚ÄúOHE‚Äù).

The n points in an n-dimensional vector, x, each refer to the offset from nn orthogonal axes. 
We can define the magnitude of the vector x to be:‚à£‚à£x‚à£‚à£ = sqrt {(x_1^2 + x_2^2 + ... + x_n^2).
The magnitude is the square root of the sum of each component squared.

Normalizing a vector means that we‚Äôre mapping the vector to a point on the unit.
v^ = (v1,...,vn)/‚à£v‚à£=( v1/‚à£v‚à£,..., vn/‚à£v‚à£)

matrices are written as nested lists, just as you‚Äôd write them in NumPy. So, the first nested list is the first row.
 
This is a new way of thinking about matrices. Instead of just storing data points, we‚Äôre now using matrices to store systems of equations.A unit circle in three dimensions is a unit sphere. In higher dimensions, it‚Äôs a unit hypersphere.

to add: np.array
dot product: np.dot(x,y)

A set is a collection of unordered, unique elements.There are two approaches to use when creating a set:
-Curly braces
-Using the built-in set() function with an iterable as an argument
Taking advantage of sets means your program runs faster and uses less memory. If you don't have duplicates and you don't need order, use a set.

there are three types of probability:
Marginal probability: The likelihood of a single event occurring, independent of any other events happening before or after it.
Joint probability: The likelihood of two independent events happening together, where the occurrence of one does not affect the occurrence of the other.
Conditional probability: The likelihood of two events happening together, where the occurrence of one affects the occurrence of the other.

the equals sign (or, in Python speak, the assignment operator) assigns the value to each variable.

Depending on how the sequence 01000001 is read, it can represent either the integer 65 or the character 'A' . 
Clearly, we need a way of telling the computer how to read the sequence. That‚Äôs where data types come in.

data types: numbers,none,boolean,strings,lists,tuples,sets,dictionaries

/	Float division
//	Integer division
%	The modulo is used to get the remainder of a division equation.

To indicate a string in Python code, simply enclose it with either ‚Äòsingle‚Äô or ‚Äúdouble‚Äù quotation marks.

The + operator concatenates, or combines, two strings together to make one big string.
* works the same. Eg 'hello'*3 will give us ‚Äúhellohellohello‚Äù

Escape characters are characters with special meanings, and they‚Äôre represented by a backslash followed by a character.
Escape characters are not a data type, but you‚Äôll still likely see them in strings
\n	Creates a new line. Eg 'Line One\nLine Two'
\t	Indents text. Eg. '\t- Bullet 1'

to print out a good old-fashioned backslash instead of reading it as an escape character: just write \\

The last of the primitive data types we‚Äôll cover is none. This data type represents a null value, or the absence of data, and it is not interchangeable with 0.

datatype conversion:
float()
int()
str()

Lists are:
Ordered: Their elements have a particular order that will never change.
Heterogeneous: Different data types can be stored for each element in the list. For example, ['cat', 10, 0.4].
Mutable: When you alter a list, you don't create a new element ‚Äî the original element is just modified.

pop() removes an element from a list. You can provide it the specific index to remove, or else it will default to removing the last element from the list.
append() adds an item to the end of a list.

tuples are similar to lists in that you can store multiple values. However, there's one huge difference:
Tuples are immutable ‚Äî you can't alter a tuple element once it's been created.
Additionally, because tuples have fixed sizes (determined when they're assigned initial values), they're more memory-efficient than a list, which needs additional memory allocated to it.
To define a new tuple, use parentheses instead of brackets
Tuples are used for information that won't change: the days of the week

To check whether an element is in a set, we use the in operator.
Eg 'Riyadh' in my_places_traveled 
True

As with sets, a dictionary is defined using curly braces. 
However, each element of a dictionary consists of a key, followed by a colon, then by a value.

To create an empty set, use the built-in set() function (e.g., untasty_fruits = set()). Why? Because in Python, curly braces are used for both sets and dictionaries. Empty braces {} indicate an empty dictionary.

len() only count unique values in sets. list everything counted

One basic type of error is a NameError. A NameError is thrown when the variable that‚Äôs referred to doesn‚Äôt exist in the namespace.

A SyntaxError (quite possibly the most common type of error) indicates that something you wrote doesn‚Äôt follow the proper Python syntax.
A SyntaxError means we‚Äôve mistyped something somewhere and, as a result, Python doesn‚Äôt understand what we‚Äôre trying to do.
Python even tries to help us troubleshoot! An arrow (^) under the offending section of code indicates where the problem occurred.

A TypeError occurs when we try to manipulate data types in a way that Python does not permit, such as adding a string and an integer or trying to get the length of an integer.

The try command lets you test a block of code for errors.

The except keyword can help you catch any exception that could occur in a try statement.

The raise keyword can be used to alert a user of a specific error type.
When the error is raised, the string entered as an argument to ValueError is the message that‚Äôs returned to the user.
Eg.if number == 4:
        raise ValueError('No fours allowed!!')
    else:
    
Def function: Def name_of_function(arguments):
A function will terminate once it reaches a return statement. 
If the return statement is not specified in a function definition, the function will return None.
Add any number of arguments separated by a comma within the parentheses.
Functions can even call other functions

calling the function:assign values to all arguments.
Eg print(my_function(3))
 
The GitHub Flow:
The workflow for contributing to an open-source product or your dev team‚Äôs project comprises the following steps:
1)Forking
2)Cloning
3)Editing
4)Adding/committing
5)Pushing
6)Submitting a pull request

Step 1: Forking
To add a copy of someone else‚Äôs GitHub repository to your GitHub account, fork it by clicking the Fork button in the upper right-hand corner.
This forked repository is not perfectly identical, but it includes all of the same source files, issues, and commit history.

Step 2: Cloning
To make a local copy of a fork, you‚Äôll clone the repository. This will save the code on your machine so you can edit it.
To do so, open your terminal, navigate to where you‚Äôd like to store the repository, then type:
git clone https://url-to-clone
You can find the URL to clone by clicking the green button that says ‚ÄúClone or download.‚Äù
Hint: If you‚Äôre following along in Git Bash on Windows, the commands to copy and paste a repository are a little different than the default Windows copy/paste commands. Use control + insert to copy and shift + insert to paste.

Step 3: Editing
You‚Äôll do this using a text editor of your choice. Atom and Sublime are some of the most popular.

Step 4: Adding and Committing
Remember, you‚Äôre editing the code on your local copy of the repository. We know that any time we do this, we need to use some very important Git commands so that our local copy is protected if we goof up.
$ git add <your-file-name>
$ git commit -m "message"

Step 5: Pushing
Once you‚Äôve committed these changes, your local repository will differ from your remote repository.
To update your remote repository on GitHub, you have to push those changes using the git push origin master command.

You don't need to worry about the origin and master part just yet. However, if you‚Äôre curious, here‚Äôs a brief overview:
origin is a shortcut for the URL of your default remote repository (in this case, the repository on GitHub). You can have many remotes if you want, but we‚Äôre only going to work with one for now.
master refers to the branch on your remote repository where you are currently adding your changes. Again, for now, we‚Äôre just going to be working on the master branch.

Step 6: Submitting a Pull Request
At this point, your local and remote repositories contain the changes you‚Äôve made. If you want to share these changes with the original repository owner, you can submit a pull request.
A pull request effectively says, ‚ÄúHello, maintainer of Project X. I made some changes here in my forked copy, and I think they‚Äôre good ones. You should add them to your repository.‚Äù
Pull requests are a GitHub feature, so you‚Äôll need to head back to the browser to submit them.

Assembly is a sequence of instructions written by a programmer, which is then translated by an assembler into the 1s and 0s that computer hardware can recognize.
Developers and programmers can choose from thousands of programming languages that are easier to read and write than assembly languages, including C++, Java, JavaScript, and Python.

All computers speak to the world through an API, an application programming interface.
In the case of the earliest computers, such as the difference engine, the API was a hand crank and the output was achieved through a series of mechanical wheels on the device that the operator would need to interpret.
Modern computers and most programs communicate with the world through a graphical user interface (GUI, pronounced like ‚Äúgooey‚Äù). You as the user input information using a keyboard and mouse, and the result is displayed on the screen. A GUI is an API that‚Äôs intended to be used by human beings.
By contrast, the command line interface (CLI) ‚Äî also called a terminal ‚Äî is a software application that interfaces with program APIs more directly, using text instead of graphics.

command + tab on Mac and alt + tab on Windows. This will allow you to quickly toggle between the browser on which you‚Äôre viewing this lesson and your CLI.

On a Mac, press command + space to bring up the spotlight search. Type in ‚Äúterminal‚Äù and press return.On Windows, go to the start menu, type ‚ÄúGit Bash‚Äù into the search, then open the application.

Git: programme for version control

In programming speak, all folders are called directories. A directory within another directory is called a subdirectory. A directory that contains a subdirectory is called a parent directory.
By default, our terminal starts in what is referred to as the home directory.
For Mac, it is /Users/yourname/.
For Windows, it is c:\users\yourname.
For Linux, it is /home/yourname.

The prompt is the $ that automatically shows up at the end of the first line. It‚Äôs the command line equivalent of ‚Äústandby‚Äù and indicates that the terminal is ready to accept your command.
The cursor follows the prompt. This is where the text you type will appear, just like in any other setting in which you‚Äôve seen a cursor.
The username of the person logged in precedes the prompt.

The pwd command stands for ‚Äúprint working directory.‚Äù 
To find out which files are in our current directory, type ls, short for ‚Äúlist.‚Äù
To change directories, we‚Äôll use cd ‚Äî ‚Äúchange directory‚Äù ‚Äî plus the name of the directory to which we want to change. Simple enough!
Eg cd Documents

Operating systems and installed applications require lots of hidden files that aren‚Äôt always relevant to everyday users. But there will be cases where, as a programmer, you‚Äôll want to view them.
Flag, which is an additional command argument that modifies the behavior of the base command.
Flags start with the - prefix.
Type ls -a, which is the list command followed by the -a flag. This means, ‚ÄúShow me all of the files in my working directory and do not ignore entries that start with a period.‚Äù

return to our parent directory. To do so, we use the cd command followed by a space and two dots:
cd ..
The dots imply ‚Äúparent directory.‚Äù
Now, if we type pwd, we‚Äôll see that we‚Äôre in our home directory, which may look like /Users/yourname if you‚Äôre on a Mac.

mkdir nameoffolder :make new folder

If we were deeper in our file structure, we could use the cd ~ command.
The tilde (~) is a shortcut for the home directory of the terminal‚Äôs current user.

we‚Äôll create a new file.
Say we want to make HTML and CSS files ‚Äî the beginnings of a website!
To accomplish this, we‚Äôll use the touch command. We can even make multiple files and file types at the same time by separating them with a space, like this:
touch index.html style.css

Removing Files
Be careful when using rm. Unlike moving files to the trash or recycle bin, deleting files with rm removes them permanently!
Eg rm -r ~/sales

To remove directories as well. 
Type rm -r myfolder to remove the myfolder directory.
-r:It stands for recursive and states that we will remove the directory along with any subdirectories or child directories. It is impossible to have a child directory without a parent directory, therefore the -r flag is always required when removing directories.
Another option, assuming the directory is empty, is rmdir, which is functionally equivalent to rm -r when executed against an empty directory.

Initializing:
$ git init
To take advantage of Git superpowers, we have to add a hidden directory called .git/ to our project directory, which contains all of the data Git needs to operate.

git status: which asks Git to give us an update on our project‚Äôs status.

To add change to your next commit, you'll use the git add command.
$ git add my-first-post.txt
The command is add, but we describe the operation by saying that the file has been ‚Äústaged.‚Äù In other words, it has been added to the list of changes that will be officially saved with our next commit.
The files on this list aren‚Äôt final, and any of these changes can be removed, or ‚Äúunstaged.‚Äù

add all of the files in the working directory to the next commit:
Instead of specifying each file, you can write git add .
Proceed with caution when using git add ., as you could accidentally add files with sensitive information.

To officially record this version of our project, type:
$ git commit -m "created a new post.txt file"
The -m option allows you to include a message that describes the changes you made for your collaborators or future you.
These should be short but descriptive and clearly indicate what changes each commit makes to the project.

working area->staging->local repo

Git allows you to add changes to your project in the local repository with two steps:
$ git add <your-file-name>
$ git commit -m "message"

SHA: it allows developers to view a list of commits, the submission date, the author, the commit message, and a unique number that identifies the commit

To view the timeline of changes, you can run:
$ git log
This will yield a list of entries that looks like this:

personal:
git init
git config user.name "someone"                  (add -global to do for account)
git config user.email "someone@someplace.com".  (add -global to do for account)

Before running git init, make sure you‚Äôre not already inside another Git repository. 
Type git status. 
If you see fatal: Not a git repository (or any of the parent directories): .git, then you know you‚Äôre good to go and you can safely run git init within this folder.

To save the changes that we‚Äôve made to our work to the GitHub platform, we must:
-Stage all of the files using git add.
-Commit our changes using git commit -m ‚Äúmessage‚Äù.
-Push the committed changes up to GitHub using git push.

Cloning is the process of downloading a repo from a remote location (most often GitHub) to our machine.
We can clone repos from our own account or even public repos from other developers and data scientists.
You may hear the phrase ‚Äúfork and clone,‚Äù which is your cue to:
Navigate to the appropriate repo on your cohort‚Äôs GitHub organization.
Fork the repo using the ‚ÄúFork‚Äù button on the repo‚Äôs page.
Run the git clone <repo link> command in your terminal to download the repo to your machine.

Codewars, a site that‚Äôs jam-packed with community-sourced programming challenges ranging in difficulty from easy to very difficult.

enumerate: can use for position
reverse list: list[::-1]
string repeat: number_of_repeat*string
sort: sorted(list)
split: .split(" ")
exclude last item: item= item[0:-1]
check even or odd: if len(item)%2 ==1:
loop every item:for i in range(0,len(ar))
loop every 2 item:for i in range(0,len(ar),2):
sort_side: dont use this. will get name error
combine strings: join

Anaconda includes many of the Python packages we‚Äôll be using in-class:
‚óã Python
‚óã iPython / Jupyter: Required tools for creating notebooks.
‚óã Pandas: Your go-to library for organizing and managing data.
‚óã Matplotlib: The king of all python plotting packages.
‚óã Gensim: Framework for vector modeling.
‚óã NLTK: Used for natural language processing.
‚óã NumPy: Fundamental array processing tool.
‚óã Scikit-Learn: Modules for machine learning & data modeling.
‚óã SciPy: Scientific library for python.
‚óã Seaborn: Statistical data visualizer.
‚óã Pip & Setuptools: package installer & version manager (Mac only).
‚óã PyMC: Common stats tool for simulation and optimization.
‚óã Sqlite: Standalone, lightweight SQL database engine.
‚óã Statsmodels: Simple statistical computation (used with SciPy).


--------------------------------------------MONDAY 6 APRIL -----------------------------------------------------
to set your account's default identity:
git config --global user.email "you@example.com"
git config --global user.name "Your Name"
Omit --global to set the identity only in this repository.

type "jupyter notebook" into the terminal at the area you working with to open juypter

shift+enter to run and move to next cell in juypter
ctl+enter to run and stay in same sell in juypter

# Exponentiation (do NOT use ^): is **
2**4

in slack channel: use ``` to type code and ``` to end, so looks nicer

can press Tab to auto complete for variable. 
after a variable., it will show the available commands for the datatype too

unpacking can be done for Tuple. list is possible but rarely done.
Eg instructor = ("Tim", "Book")
first, last = instructor

.add: add alphabetically

check if something inside: use in
Eg 'B+' in my_grades
True

can have nested dictionary

markdown:
* italics
** bold
### heading

The print statement removes the quotations, whereas just running they jupyter cell with x at the last line leaves the quotations in.

print(f"   ")

press stop to stop infinite loop in jupyter notebook

numpy array deals with homogenous data collection unlike lists

variable.method(parameters)

join using _: Eg names_all= "_".join(names)
insert new value into list at a certain position: Eg. names.insert(4,"Elon")

escape character: to use quotations in quotations: Eg \"Kings\"

Info from input is string
So use float(input()) if want float

round(,2) don‚Äôt give 2 dp if exact ans. 
Decimal package then give

Shift + tab to take find purpose of method/when calling function can see what are the parameters

Np array has to be homogenous values. Eg all int or all float 


--------------------------------------------TUESDAY 7 APRIL -----------------------------------------------------
Default value: Only if no parameter passed then default used.
Cannot put Default value for argument in first argument. doesn't seem to work

Function only reads variable defined inside

changing global variable inside a function:
g_var2 = "Outside"
def function3():
    global g_var2 # explicitly declare that the scope of this variable is global
    g_var2 = "Inside"

Split method by default look for a space. Doesn‚Äôt split when no space
--------------------------------------------WEDNESDAY 8 APRIL -----------------------------------------------------
Don‚Äôt append or remove the main list while trying to run a for loop. Will cause the length of the list to change

Sample space is all outcomes of the experiment

Event: collection of outcome of experiment: Eg. A = Event of getting 2 heads
--------------------------------------------THURSDAY 9 APRIL -----------------------------------------------------
Distribution: frequency of all possible outcomes

Distribution illustration and formula: https://seeing-theory.brown.edu/probability-distributions/index.html#section2

Probability mass function shows us the probability that our random variable takes on a specific value.

Cumulative distribution function shows us the probability that our random variable takes on any value less than or equal to a specific value.
--------------------------------------------MONDAY 13 APRIL -----------------------------------------------------
--------------------------------------------INTRO TO PANDAS------------------------------------------------------
methods:
.shape
.columns
.read_csv() : Can read a specific number of rows using nrows=
.describe()
.info()
.head()
.tail()
.value_counts() : applied to columns, will count number of each categorical variable
.mean()
.median()
.quantile()
.to_datetime() : to convert to datatime data type
.isin( )
.between( , )
.sort_values()
.sort_values([ , ]) : on 2 columns
np.mean()
np.log()
.groupyby : eg df.groupby('var1')['var2'].agg(['count', 'mean', function])
.drop( ,axis=1,inplace=True) : axis=0 is rows. axis=1 is column. inplace=True replace original data. otherwise original df not affected
.columns.str.lower() :  eg df.columns.str.lower().  : lowercase headers. do note data not replaced
.str.lower() :  eg df['var'].str.lower() :l owercase headers. do note data not replaced
.rename() :  eg df.rename(columns={"colors reported": "colors", "shape reported":"shape"},inplace=True).  : rename some
.columns  : eg df.columns = ['city','colors','shape','state','time'].  : rename all
.str.replace(,) :  eg df['var'].str.replace('o', 'bro').  : replace values in df
.dt.year :  eg df['time'].dt.year.  : take out year from datatime data type
.isnull()
.notnull()
.loc : eg df.loc[df['var1'].notnull(),:] : filter out missing values
.to csv() : export data. eg. df.loc[df['var']>30,:].to_csv('datasets/highmpgcars.csv',index=False)
.merge(,,) :  eg pd.merge(df1, df2, how='left')  : merge to the main dataframe1
.crosstab(,) :  eg pd.crosstab(df['var1'], df['var2'])
.Categorical(,) : eg pd.Categorical(df['var1'], categories=['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']) : reorder categories
.map() :  eg df['var2'] = df['var1'].map({4: 'Four', 6: 'Six', 8: 'Eight'}) or df['var2'] = df['var1'].map(function) or df['var2'] = df['var1'].map(lambda x: "Efficient" if x>20 else "Wasteful")
.apply(function, axis=1). : axis=0 is rows. axis=1 is column. 
.map

Others:
plt.figure(figsize=( , ))
plt.hist( , bins=)
np.array([ , , , ,...])
v[v<40]:return values that fulfil condition
pd.Series([ , , ..])  : creata panda series. not homogenous
plt.subplot(,,) : decide number of plots will be drawn
sorted(list) returns a list. Capital letter sorted first.
list.sort() don‚Äôt return a list. It changes the list

dataframe (panda object. data container):
df[ ] or df.'' : one column
df[ ][ ]: one cell
df[' ']<40
df[(df[' ']==4) | (df[' ']==1)]
~ df[ ]: not
df[[' ']] into dataframe. avoid just df[' ']
loc and iloc to filter rows and columns both:
df.loc[(df[' ']==4) | (df[' ']==1), [ ,  ,  ,  ...]]
df.iloc[ , ]

panda gives ordinal instead of simply categorical data
map is for dataseries. apply for dataframe. map and apply same if apply for dataseries
--------------------------------------------TUESDAY 14 APRIL -----------------------------------------------------
--------------------------------------------DATA VISUALISATION------------------------------------------------------
method:
.dtypes
.index
.sort_values
.unique #panda unique. or np.unique from numpy
.sort_index
.plot(kind='') #kind=bar/barh/hist/box/scatter
sns.pairplot() #same as the one below
pd.plotting.scatter_matrix(df[ , ])
sns.distplot()
sns.heatmap(df.corr)
plt.style.use() #stylesheet
random.sample: is by python. no replace
np.random.choice: is by np. got replace

others:
-for boxplot, "min" line is Q1-1.5*IQR, "max" line is Q3+1.5*IQR
-text viz is also good to emphasise just one value
-choose visual that bring across the message
-have slides with just graph and no text describing them. explain the graph instead
-ideally use [] to refer to df columns instead of

--------------------------------------------WEDNESDAY 15 APRIL -----------------------------------------------------
-----------------------------------------EXPLORATORY DATA ANALYSIS--------------------------------------------------
methods:
.copy : to copy to a new variable instead of pointing to the same variable
.dropna()
.astype()
sns.boxplot
.dtypes()
plt.figure()
.isnull().sum()

others:
.() : is like a method whereas . is like an attribute

--------------------------------------------THURSDAY 16 APRIL -----------------------------------------------------
--------------------------------CONFIDENCE INTERVAL AND HYPOTHESIS TESTING-------------------------------------------
methods:
.var : variance
.tolist
pd.get_dummies() : make dummies from values

to setup folder to push:
1) create repo on github
2) git init
3) git add .                            : . is to add all files. to add specific type filename.
4) git commit -m "comment"
5) git remote add origin https://...
6) git remote -v                        : is to check if file pointing correct
7) git push -u origin master

others:
-np.NaN : null value
-masking: df[(...==...) | (...==...)]
-H0 is about the population. We draw sample to make inference about the population.
-standard deviation of sampling distribution = standard error

--------------------------------------------FRIDAY 17 APRIL -----------------------------------------------------
--------------------------------ADVANCED TRANSFORMATION USING PANDAS---------------------------------------------
df.values = array : df to array
df.sum() : give column. aggregation over entire dataset
  .mean() : aggregation over entire dataset
pd.melt(df: id_var=[ , ]). :put df into 2 column. id_var is those column you don't want to melt
melted.pivot():  melted=pd.melt(df)
.reset_index()

Others:
-for , in enumerate(): : to access array
- .map for series only. lambda takes each values

Groupby:
df.groupby().describe()
            .sum() :aggregation with group + pandas. aggregation over sepcific variables
            .mean() :aggregation with group + pandas. aggregation over sepcific variables
            .agg({variable:[np.mean,np.median]}). :aggregation with group + pandas. aggregation over sepcific variables
            .count() : if any group specifically high, may not be good to use this variable
            
After Groupby:
group.get_group("groupname") : get subset of dataframe. access each group. 
.groups : shows groups in groupby by index.
for groupname,groupdf in category: : iterate over the groups

--------------------------------------------TUESDAY 21 APRIL -----------------------------------------------------
--------------------------------------------LINEAR REGRESSION-----------------------------------------------------
### Plot scatter
# create a figure
fig = plt.figure(figsize=(12,7))
# get current axis of that figure
ax = fig.gca()
# plot a scatter plot on it with our data
ax.scatter(x,y);
#plot line of fit
ax.plot(df['X'], df['Linear_Yhat'], color='k');
#plot baseline 
ax.plot((df['X'].min(),df['X'].max()),(np.mean(df['Y']),np.mean(df['Y'])),color='r');
# iterate over predictions
for _, row in df.iterrows():
    plt.plot((row['X'], row['X']), (row['Y'], row['Linear_Yhat']), 'r-')
    
Methods:
np.std(df['X'],ddof=1) : get sd
r_xy = df.corr().loc['X','Y']   or np.corrcoef(df['X'],df['Y'])[1][0]. #get corr
df.loc[:,['var1','var2',..]] : get the columns you want
.values : get values from df
X.T : The transpose of a matrix is calculated by appending .T to the matrix
np.dot(mat1, mat2): Matrices multipled in the formula should be done with the "dot product"
np.linalg.inv(): Inverting a matrix is done using



Goal:
minimize loss function: where the "loss" here is the sum of squared residuals or or residual sum of squares (RSS) or SSE (sum of squared errors).
 RSS = sum{yi-yi_hat}^2  
 code: np.sum(np.square(df['Y'] - df['Mean_yhat']))
 equations for the intercept and slope that minimize the RSS is: ùõΩÃÇ_0_hat= ùë¶_bar ‚àí (ùõΩÃÇ1_hat * x_bar)
                                                                 ùõΩÃÇ_1_hat= (ùëü_ùëãùëå * ùë†ùëå ) / ùë†ùëã
  such that:
  ùë¶_bar  : the sample mean of observed values  ùëå 
  ùë•_bar : the sample mean of observed values  ùëã 
  ùë†ùëå  : the sample standard deviation of observed values  ùëå 
  ùë†ùëã  : the sample standard deviation of observed values  ùëã 
  ùëüùëãùëå  : the sample Pearson correlation coefficient between observed  ùëã  and  ùëå
  
  for multiple linear regression to minimize sse: ùõΩ = (ùëãùëá * ùëã)^-1 * ùëãùëá* ùë¶
  
  to get betas: 
  from sklearn.linear_model import LinearRegression
  linreg = LinearRegression()
  linreg.fit(X, price)
  print(linreg.coef_)

Common metrics for evaluating regression models:
- MSE
  np.mean(np.square(df['Y']-df['Mean_yhat']))
- root mean squared error (RMSE) is a standard measure of model performance. 
  It is the square root of the mean of the sum of squared residuals:
  RMSE= sqrt (1/ùëõ ‚àëùëñ(ùë¶ùëñ_hat‚àíùë¶ùëñ)^2)
 The smaller the root mean squared error, the better your model fits the data.
 code: from sklearn.metrics import mean_squared_error, r2_score
       print(np.sqrt(mean_squared_error(target, predictions))) 
-coefficient of determination  ùëÖ2 : 
 ùëÖ2=1‚àíùëÜùëÜùëüùëíùëî/ùëÜùëÜùë°ùëúùë° 
Where the regression sum of squares is the sum of squared residuals for our model:
ùëÜùëÜùëüùëíùëî=‚àëùëñ(y_hat ‚àíùë¶i)2 
And the total sum of squares is the sum of squared residuals for the baseline model. This is essentially the variance of our target.
ùëÜùëÜùë°ùëúùë°=‚àëùëñ(ùë¶ùëñ‚àíùë¶_bar)2 
ùëÖ^2  is the most common metric to evaluate a regression and is the default scoring measure in sklearn. When we cover classification models, the .score function instead defaults to accuracy.
A negative  ùëÖ2  means that the regression model is performing worse than the baseline model. 

Others:
-residual ùúÄùëñ = yi-yi_hat. residuals = y-predictions
-simple linear regression is an estimator of the expected value (mean) of ùëå
-y_hat = ùõΩÃÇ_0_hat + ùõΩÃÇ_1_hat ùëã
-predict with model: df['Linear_Yhat'] = beta_0 + beta_1 * df['X']

-The simple linear regression estimators described above have really nice properties:
They are unbiased. (a.k.a. the statistics will on average equal the true parameter value)
Among all unbiased estimators, the estimators above have the smallest possible variance.
They are consistent. (a.k.a. as your sample size n goes to infinity, the statistic converges to the true parameter value)
However, these are just mathematical properties. They do not take into account real-world situations. F
When building a model, make sure your model makes sense! You are responsible for the interpretation of the model. All the computer will do is optimize. 

-There are some critical assumptions involved in simple linear regression that you must be aware of:
Linearity: Y and X must have an approximately linear relationship.
Independence: Errors (residuals) ùúÄùëñ and ùúÄùëó must be independent of one another for any ùëñ‚â†ùëó.
Normality: The errors (residuals) follow a Normal distribution with mean 0.
Equality of Variances (Homoscedasticity of errors): The errors (residuals) should have a roughly consistent pattern, regardless of the value of X. (There should be no discernable relationship between X and the residuals.)
The mnemonic LINE is a useful way to remember these four assumptions.
If all four assumptions are true, the following holds: ùëåùëñ‚àºùëÅ(ùõΩ0+ùõΩ1ùëãùëñ,ùúé)

-sklearn is the machine learning package
statsmodels is the statistics package
Though the terms have immense overlap, machine learning tends to be more prediction focused while statistics is more inference focused.

Steps for linear regression using sklearn:
1) #from sklearn import linear_model
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
2) # initiate model
model = LinearRegression()
3) # fit the model
X = df[['var1']]
y = target
model.fit(X,y)
predictions  =  model.predict(X)
4) score        =  model.score(X, y)  #a class method / function that returns the coefficient of determination R^2 of the prediction (for regression models).
# Plot the model
plt.figure(figsize=(12,7))
plt.scatter(predictions,y,c='r')
plt.xlabel('....')
plt.ylabel('....');
#get attributes and interpret them
model.coef_
model.intercept_
#evaluate:
from sklearn.metrics import mean_squared_error, r2_score
print(np.sqrt(mean_squared_error(target, predictions))) 
print(r2_score(target, predictions)) #model.score also can


fit MLR example:
lm = linear_model.LinearRegression()
# two predictors
X = df[['carat','table']].values
y = target
model = lm.fit(X, y)
predictions = model.predict(X)
score = model.score(X, y)
# Plot the model
plt.figure(figsize=(8,8))
plt.scatter(predictions, y, s=30, c='r', marker='+', zorder=10)
plt.xlabel("Predicted Values from CARAT + TABLE - $\hat{y}$")
plt.ylabel("Actual Values PRICE - y")
plt.show()
print("score: ", score)

Fitting a linear regression using statsmodels:
NOTE:  The statsmodels process is slightly different:
We manually make a new column for the intercept in our design matrix  ùëã .
The  ùë¶  target variable comes before the  ùëã  predictor
The data is provided during the instantiation of the model object, then fit is called without the data.
code:
import statsmodels.api as sm
X = df[[['carat','table']].values
# manually add the intercept column:
X = sm.add_constant(X)
y = target
model = sm.OLS(y, X)
model = model.fit()
predictions = model.predict()
--------------------------------------------WEDNESDAY 22 APRIL -----------------------------------------------------
-------------------------------------------------------------------------------------------------
deal with categorical data:
-one hot encoding: .get_dummies() : convert string to multiple columns binary
-ordinal : for ordered data eg good,very good. difference is 1unit. convert string to just 1 column values

supervised learning: with target y given
1) classification: target is categorical/discrete. x can be cts or discrete
2) regression: target is cts. x can be cts or discrete

bias = difference between prediction and true relationship between the variables
variance = ability to replicate the results with similar sum of squared errors for future datasets. high var dont perform well for data that has not seen.
can be caused by variables with high collinearity
how to tell overfit: var will increase with new dataset.
solutions: regularization, bagging and boosting
--------------------------------------------THURSDAY 23 APRIL -----------------------------------------------------
------------------------------------FEATURE ENGINEERING AND REGULARIZATION-----------------------------------------
Others:
-features need to be standardized in regularized model
-Elastic net combines ridge and lasso penalties
-Ridge regression: increase lambda, target less sensitive to predictors
-Ridge is square of coefficient but lasso is absolute
-ridge can only make coefficient small but lasso can make coefficiemt zero (feature selection technique)
-overfit: dont generalize well,training the noise,low bias but high var

## select all the columns that are not the target
nc = [x for x in df.columns if x != target]

## Standardize to dataframe
df[nc] = (df[nc]-df[nc].mean())/ df[nc].std()

standardization only to x. no need to y.
--------------------------------------------FRIDAY 23 APRIL -----------------------------------------------------
-------------------------------------------MISSING DATA IMPUTATION-----------------------------------------------
correct term: data science with missing data not 'handle missing data'

3 types of missing data:
-Missing completely at random (MCAR)
-Missing at random (MAR)
-Not missing at random (NMAR)

Within the 3 types:
-item nonresponse: some values not observed
-unit nonresponse: a value not observed

3 methods to solve for missing data:
-avoid it
-ignore it
-accoount for it

to avoid unit nonresponse:
-change data collection method
-avoid burden respondent
-improve accessibility
-change timing of survey

to account for unit nonresponse:
-weight class distribution. reweight all respondents by true proportion/proportion of response

avoid item nonresponse:
-minimize length of questionnaire
-design questionnaire with respondent in mind

ignore item nonresponse:
-complete case analysis: drops obs with any missing value. cons: info loss
-avaialble case analysis: drops no obs calculate results based on available data. cons: results may be abnormal. egg corr >1

account for item nonresponse:
-deductive imputation
-inferential imputation (fill na with distribution): mean/median/model, regression, stocashtic (MAR data), hot-deck or proper imputation
-pattern submodel approach

to deduce MCAR,MAR or NMAR:
-Little'test for MCAR vs MAR. no empirical test for NMAR
-partition data for missing and non-missing and compare
-think about why data is missing and how it affects the study.

always think about data quality

if little data missing: fill them
if many, drop them
if some data missing: common approach proper imputation, stochastic imputation, pattern submodel

to interpret categorical or integer: boxplot or compare the means if drastically different. use groupby
--------------------------------------------MONDAY 27 APRIL -----------------------------------------------------
---------------------------CLASSIFICATION, LOGISTIC REGRSSION, K-NEAREST NEIGHBORS-------------------------------
unsupervised learning: no target y. 
supervised learning:
1) regression: uncountable number of outcomes y
2) classfication: countable number of outcomes y. binary/multiclass classification. eg logisitic regression
------------------------------------------------LINKS ------------------------------------------------------------
Run code: https://repl.it/
https://www.hackerrank.com/challenges/nested-list/problem
https://www.python.org/dev/peps/pep-0008/#tabs-or-spaces
https://docs.python.org/3/library/exceptions.html
https://docs.python.org/3/tutorial/errors.html
https://campus.datacamp.com/courses/kaggle-r-tutorial-on-machine-learning/chapter-3-improving-your-predictions-through-random-forests?ex=1
https://automatetheboringstuff.com/
jupyter viewer for github: https://nbviewer.jupyter.org/
--------------------------------------------TO THINK -------------------------------------------------------------
Think what is enumerate again (practice odds_and_even)
Diff between lambda and list comprehension
Can use?  if roll_1 > roll_2 > roll_3
Is control flow same topic as functions
PEP guide -intentation don‚Äôt understand ‚Äúhanging indent should add a a level‚Äù
Cannot remove directory/folder
Why 0.2+0.1 != 0.3
What is this??
def find_longest_word(list_of_words):
    return len(max(list_of_words, key=len))
    
to read:
https://jakevdp.github.io/PythonDataScienceHandbook/03.04-missing-values.html
https://www3.ntu.edu.sg/home/ehchua/programming/webprogramming/Python4_DataAnalysis.html
https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html
https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html
https://towardsdatascience.com/why-sample-variance-is-divided-by-n-1-89821b83ef6d
http://www.nohsteachers.info/pcaso/ap_statistics/PDFs/DegreesOfFreedom.pdf
https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-are-degrees-of-freedom-in-statistics
https://www.edureka.co/blog/python-regex/
https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/
a lot link in 3.01 ipy 
https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html
https://towardsdatascience.com/basic-time-series-manipulation-with-pandas-4432afee64ea
https://towardsdatascience.com/how-to-show-all-columns-rows-of-a-pandas-dataframe-c49d4507fcf
https://www.data-blogger.com/2017/11/15/python-matplotlib-pyplot-a-perfect-combination/
https://towardsdatascience.com/the-bias-variance-trade-off-explanation-and-demo-8f462f8d6326
https://readthedocs.org/projects/patsy/downloads/pdf/latest/
https://thispointer.com/python-pandas-how-to-drop-rows-in-dataframe-by-conditions-on-column-values/
