--------------------------------------------PREWORK-----------------------------------------------------
feature engineering: Create columns derived from our data

What does a compelling data-driven presentation look like?
-Summarizing your findings.
-Labeling all plots and visualizations.
-Restating your hypothesis and initial assumptions.
-Describing your data and process.
-Explaining your model’s strengths and limitations.
-Providing an appropriate degree of disclosure for your audience (especially when dealing with proprietary data or sensitive user information).

range(x): 0 to x, not inclusive of x
range(a,b,c): a=start,b=stop,c=step

One way of vectorizing your data is called label encoding — assigning numerical values to each attribute.
Another common technique for vectorizing categorical data is called one-hot encoding (sometimes abbreviated as “OHE”).

The n points in an n-dimensional vector, x, each refer to the offset from nn orthogonal axes. 
We can define the magnitude of the vector x to be:∣∣x∣∣ = sqrt {(x_1^2 + x_2^2 + ... + x_n^2).
The magnitude is the square root of the sum of each component squared.

Normalizing a vector means that we’re mapping the vector to a point on the unit.
v^ = (v1,...,vn)/∣v∣=( v1/∣v∣,..., vn/∣v∣)

matrices are written as nested lists, just as you’d write them in NumPy. So, the first nested list is the first row.
 
This is a new way of thinking about matrices. Instead of just storing data points, we’re now using matrices to store systems of equations.A unit circle in three dimensions is a unit sphere. In higher dimensions, it’s a unit hypersphere.

to add: np.array
dot product: np.dot(x,y)

A set is a collection of unordered, unique elements.There are two approaches to use when creating a set:
-Curly braces
-Using the built-in set() function with an iterable as an argument
Taking advantage of sets means your program runs faster and uses less memory. If you don't have duplicates and you don't need order, use a set.

there are three types of probability:
Marginal probability: The likelihood of a single event occurring, independent of any other events happening before or after it.
Joint probability: The likelihood of two independent events happening together, where the occurrence of one does not affect the occurrence of the other.
Conditional probability: The likelihood of two events happening together, where the occurrence of one affects the occurrence of the other.

the equals sign (or, in Python speak, the assignment operator) assigns the value to each variable.

Depending on how the sequence 01000001 is read, it can represent either the integer 65 or the character 'A' . 
Clearly, we need a way of telling the computer how to read the sequence. That’s where data types come in.

data types: numbers,none,boolean,strings,lists,tuples,sets,dictionaries

/	Float division
//	Integer division
%	The modulo is used to get the remainder of a division equation.

To indicate a string in Python code, simply enclose it with either ‘single’ or “double” quotation marks.

The + operator concatenates, or combines, two strings together to make one big string.
* works the same. Eg 'hello'*3 will give us “hellohellohello”

Escape characters are characters with special meanings, and they’re represented by a backslash followed by a character.
Escape characters are not a data type, but you’ll still likely see them in strings
\n	Creates a new line. Eg 'Line One\nLine Two'
\t	Indents text. Eg. '\t- Bullet 1'

to print out a good old-fashioned backslash instead of reading it as an escape character: just write \\

The last of the primitive data types we’ll cover is none. This data type represents a null value, or the absence of data, and it is not interchangeable with 0.

datatype conversion:
float()
int()
str()

Lists are:
Ordered: Their elements have a particular order that will never change.
Heterogeneous: Different data types can be stored for each element in the list. For example, ['cat', 10, 0.4].
Mutable: When you alter a list, you don't create a new element — the original element is just modified.

pop() removes an element from a list. You can provide it the specific index to remove, or else it will default to removing the last element from the list.
append() adds an item to the end of a list.

tuples are similar to lists in that you can store multiple values. However, there's one huge difference:
Tuples are immutable — you can't alter a tuple element once it's been created.
Additionally, because tuples have fixed sizes (determined when they're assigned initial values), they're more memory-efficient than a list, which needs additional memory allocated to it.
To define a new tuple, use parentheses instead of brackets
Tuples are used for information that won't change: the days of the week

To check whether an element is in a set, we use the in operator.
Eg 'Riyadh' in my_places_traveled 
True

As with sets, a dictionary is defined using curly braces. 
However, each element of a dictionary consists of a key, followed by a colon, then by a value.

To create an empty set, use the built-in set() function (e.g., untasty_fruits = set()). Why? Because in Python, curly braces are used for both sets and dictionaries. Empty braces {} indicate an empty dictionary.

len() only count unique values in sets. list everything counted

One basic type of error is a NameError. A NameError is thrown when the variable that’s referred to doesn’t exist in the namespace.

A SyntaxError (quite possibly the most common type of error) indicates that something you wrote doesn’t follow the proper Python syntax.
A SyntaxError means we’ve mistyped something somewhere and, as a result, Python doesn’t understand what we’re trying to do.
Python even tries to help us troubleshoot! An arrow (^) under the offending section of code indicates where the problem occurred.

A TypeError occurs when we try to manipulate data types in a way that Python does not permit, such as adding a string and an integer or trying to get the length of an integer.

The try command lets you test a block of code for errors.

The except keyword can help you catch any exception that could occur in a try statement.

The raise keyword can be used to alert a user of a specific error type.
When the error is raised, the string entered as an argument to ValueError is the message that’s returned to the user.
Eg.if number == 4:
        raise ValueError('No fours allowed!!')
    else:
    
Def function: Def name_of_function(arguments):
A function will terminate once it reaches a return statement. 
If the return statement is not specified in a function definition, the function will return None.
Add any number of arguments separated by a comma within the parentheses.
Functions can even call other functions

calling the function:assign values to all arguments.
Eg print(my_function(3))
 
The GitHub Flow:
The workflow for contributing to an open-source product or your dev team’s project comprises the following steps:
1)Forking
2)Cloning
3)Editing
4)Adding/committing
5)Pushing
6)Submitting a pull request

Step 1: Forking
To add a copy of someone else’s GitHub repository to your GitHub account, fork it by clicking the Fork button in the upper right-hand corner.
This forked repository is not perfectly identical, but it includes all of the same source files, issues, and commit history.

Step 2: Cloning
To make a local copy of a fork, you’ll clone the repository. This will save the code on your machine so you can edit it.
To do so, open your terminal, navigate to where you’d like to store the repository, then type:
git clone https://url-to-
You can find the URL to clone by clicking the green button that says “Clone or download.”
Hint: If you’re following along in Git Bash on Windows, the commands to copy and paste a repository are a little different than the default Windows copy/paste commands. Use control + insert to copy and shift + insert to paste.

Step 3: Editing
You’ll do this using a text editor of your choice. Atom and Sublime are some of the most popular.

Step 4: Adding and Committing
Remember, you’re editing the code on your local copy of the repository. We know that any time we do this, we need to use some very important Git commands so that our local copy is protected if we goof up.
$ git add <your-file-name>
$ git commit -m "message"

Step 5: Pushing
Once you’ve committed these changes, your local repository will differ from your remote repository.
To update your remote repository on GitHub, you have to push those changes using the git push origin master command.

You don't need to worry about the origin and master part just yet. However, if you’re curious, here’s a brief overview:
origin is a shortcut for the URL of your default remote repository (in this case, the repository on GitHub). You can have many remotes if you want, but we’re only going to work with one for now.
master refers to the branch on your remote repository where you are currently adding your changes. Again, for now, we’re just going to be working on the master branch.

Step 6: Submitting a Pull Request
At this point, your local and remote repositories contain the changes you’ve made. If you want to share these changes with the original repository owner, you can submit a pull request.
A pull request effectively says, “Hello, maintainer of Project X. I made some changes here in my forked copy, and I think they’re good ones. You should add them to your repository.”
Pull requests are a GitHub feature, so you’ll need to head back to the browser to submit them.

Assembly is a sequence of instructions written by a programmer, which is then translated by an assembler into the 1s and 0s that computer hardware can recognize.
Developers and programmers can choose from thousands of programming languages that are easier to read and write than assembly languages, including C++, Java, JavaScript, and Python.

All computers speak to the world through an API, an application programming interface.
In the case of the earliest computers, such as the difference engine, the API was a hand crank and the output was achieved through a series of mechanical wheels on the device that the operator would need to interpret.
Modern computers and most programs communicate with the world through a graphical user interface (GUI, pronounced like “gooey”). You as the user input information using a keyboard and mouse, and the result is displayed on the screen. A GUI is an API that’s intended to be used by human beings.
By contrast, the command line interface (CLI) — also called a terminal — is a software application that interfaces with program APIs more directly, using text instead of graphics.

command + tab on Mac and alt + tab on Windows. This will allow you to quickly toggle between the browser on which you’re viewing this lesson and your CLI.

On a Mac, press command + space to bring up the spotlight search. Type in “terminal” and press return.On Windows, go to the start menu, type “Git Bash” into the search, then open the application.

Git: programme for version control

In programming speak, all folders are called directories. A directory within another directory is called a subdirectory. A directory that contains a subdirectory is called a parent directory.
By default, our terminal starts in what is referred to as the home directory.
For Mac, it is /Users/yourname/.
For Windows, it is c:\users\yourname.
For Linux, it is /home/yourname.

The prompt is the $ that automatically shows up at the end of the first line. It’s the command line equivalent of “standby” and indicates that the terminal is ready to accept your command.
The cursor follows the prompt. This is where the text you type will appear, just like in any other setting in which you’ve seen a cursor.
The username of the person logged in precedes the prompt.

The pwd command stands for “print working directory.” 
To find out which files are in our current directory, type ls, short for “list.”
To change directories, we’ll use cd — “change directory” — plus the name of the directory to which we want to change. Simple enough!
Eg cd Documents

Operating systems and installed applications require lots of hidden files that aren’t always relevant to everyday users. But there will be cases where, as a programmer, you’ll want to view them.
Flag, which is an additional command argument that modifies the behavior of the base command.
Flags start with the - prefix.
Type ls -a, which is the list command followed by the -a flag. This means, “Show me all of the files in my working directory and do not ignore entries that start with a period.”

return to our parent directory. To do so, we use the cd command followed by a space and two dots:
cd ..
The dots imply “parent directory.”
Now, if we type pwd, we’ll see that we’re in our home directory, which may look like /Users/yourname if you’re on a Mac.

mkdir nameoffolder :make new folder

If we were deeper in our file structure, we could use the cd ~ command.
The tilde (~) is a shortcut for the home directory of the terminal’s current user.

we’ll create a new file.
Say we want to make HTML and CSS files — the beginnings of a website!
To accomplish this, we’ll use the touch command. We can even make multiple files and file types at the same time by separating them with a space, like this:
touch index.html style.css

Removing Files
Be careful when using rm. Unlike moving files to the trash or recycle bin, deleting files with rm removes them permanently!
Eg rm -r ~/sales
or rm -r /Users/name/sales

To remove directories as well. 
Type rm -r myfolder to remove the myfolder directory.
-r:It stands for recursive and states that we will remove the directory along with any subdirectories or child directories. It is impossible to have a child directory without a parent directory, therefore the -r flag is always required when removing directories.
Another option, assuming the directory is empty, is rmdir, which is functionally equivalent to rm -r when executed against an empty directory.

Initializing:
$ git init
To take advantage of Git superpowers, we have to add a hidden directory called .git/ to our project directory, which contains all of the data Git needs to operate.

git status: which asks Git to give us an update on our project’s status.

To add change to your next commit, you'll use the git add command.
$ git add my-first-post.txt
The command is add, but we describe the operation by saying that the file has been “staged.” In other words, it has been added to the list of changes that will be officially saved with our next commit.
The files on this list aren’t final, and any of these changes can be removed, or “unstaged.”

add all of the files in the working directory to the next commit:
Instead of specifying each file, you can write git add .
Proceed with caution when using git add ., as you could accidentally add files with sensitive information.

To officially record this version of our project, type:
$ git commit -m "created a new post.txt file"
The -m option allows you to include a message that describes the changes you made for your collaborators or future you.
These should be short but descriptive and clearly indicate what changes each commit makes to the project.

working area->staging->local repo

Git allows you to add changes to your project in the local repository with two steps:
$ git add <your-file-name>
$ git commit -m "message"

SHA: it allows developers to view a list of commits, the submission date, the author, the commit message, and a unique number that identifies the commit

To view the timeline of changes, you can run:
$ git log
This will yield a list of entries that looks like this:

personal:
git init
git config user.name "someone"                  (add -global to do for account)
git config user.email "someone@someplace.com".  (add -global to do for account)

Before running git init, make sure you’re not already inside another Git repository. 
Type git status. 
If you see fatal: Not a git repository (or any of the parent directories): .git, then you know you’re good to go and you can safely run git init within this folder.

To save the changes that we’ve made to our work to the GitHub platform, we must:
-Stage all of the files using git add.
-Commit our changes using git commit -m “message”.
-Push the committed changes up to GitHub using git push.

Cloning is the process of downloading a repo from a remote location (most often GitHub) to our machine.
We can clone repos from our own account or even public repos from other developers and data scientists.
You may hear the phrase “fork and clone,” which is your cue to:
Navigate to the appropriate repo on your cohort’s GitHub organization.
Fork the repo using the “Fork” button on the repo’s page.
Run the git clone <repo link> command in your terminal to download the repo to your machine.

Codewars, a site that’s jam-packed with community-sourced programming challenges ranging in difficulty from easy to very difficult.

enumerate: can use for position
reverse list: list[::-1]
string repeat: number_of_repeat*string
sort: sorted(list)
split: .split(" ")
exclude last item: item= item[0:-1]
check even or odd: if len(item)%2 ==1:
loop every item:for i in range(0,len(ar))
loop every 2 item:for i in range(0,len(ar),2):
sort_side: dont use this. will get name error
combine strings: join

Anaconda includes many of the Python packages we’ll be using in-class:
○ Python
○ iPython / Jupyter: Required tools for creating notebooks.
○ Pandas: Your go-to library for organizing and managing data.
○ Matplotlib: The king of all python plotting packages.
○ Gensim: Framework for vector modeling.
○ NLTK: Used for natural language processing.
○ NumPy: Fundamental array processing tool.
○ Scikit-Learn: Modules for machine learning & data modeling.
○ SciPy: Scientific library for python.
○ Seaborn: Statistical data visualizer.
○ Pip & Setuptools: package installer & version manager (Mac only).
○ PyMC: Common stats tool for simulation and optimization.
○ Sqlite: Standalone, lightweight SQL database engine.
○ Statsmodels: Simple statistical computation (used with SciPy).

---------------------------------------------WEEK 1 DAY 1 -------------------------------------------------------------
--------------------------------------------MONDAY 6 APRIL ------------------------------------------------------------
----------------------------------------CONTROL FLOW AND DATATYPES-----------------------------------------------------
to set your account's default identity:
git config --global user.email "you@example.com"
git config --global user.name "Your Name"
Omit --global to set the identity only in this repository.

type "jupyter notebook" into the terminal at the area you working with to open juypter

shift+enter to run and move to next cell in juypter
ctl+enter to run and stay in same sell in juypter

# Exponentiation (do NOT use ^): is **
2**4

in slack channel: use ``` to type code and ``` to end, so looks nicer

can press Tab to auto complete for variable. 
after a variable., it will show the available commands for the datatype too

unpacking can be done for Tuple. list is possible but rarely done.
Eg instructor = ("Tim", "Book")
first, last = instructor

.add: add alphabetically

check if something inside: use in
Eg 'B+' in my_grades
True

can have nested dictionary

markdown:
* italics
** bold
### heading

The print statement removes the quotations, whereas just running they jupyter cell with x at the last line leaves the quotations in.

print(f"   ")

press stop to stop infinite loop in jupyter notebook

numpy array deals with homogenous data collection unlike lists

variable.method(parameters)

join using _: Eg names_all= "_".join(names)
insert new value into list at a certain position: Eg. names.insert(4,"Elon")

escape character: to use quotations in quotations: Eg \"Kings\"

Info from input is string
So use float(input()) if want float

round(,2) don’t give 2 dp if exact ans. 
Decimal package then give

Shift + tab to take find purpose of method/when calling function can see what are the parameters

Np array has to be homogenous values. Eg all int or all float 

Tuple usually unpack instead of  access using index. Good practice
instructor = ("Tim", "Book")
first, last = instructor

Unpack can use for lists also but not common. usually use index instead

slecting list elements = slicing/indexing
can slice strings also.

datatype: type()

athimetic:
mod: %
expo: **
floor division: //

String
.split(' ')[1]
.count()
.replace(,)

Sets
.elements in set are immutable. hence a tuple may be inside a set but not dict or lists
.Sets are unordered.
.Sets are in alphabetical order when called
.Set elements are unique
.can have different types in a set
.union()
.intersection()
.add()
.remove()
.Strings are also iterable, so a string can be passed to set() as well. set(s) generates a set of the characters in s:
s = 'quux'
set(s)
{'q', 'u', 'x'}

Lists
“Test”.join(list)
.append()
.remove()
.Backwards: list[::-1]

Boolean
Strings can also be used with Boolean operators. They are case-sensitive.
print("Sammy == sammy: ", Sammy == sammy)
Sammy == sammy:  False

Tuples
* Immutable 
* Heterogeneous 
Tuples have no append or extend method.
Elements cannot be removed from a tuple.
You can find elements in a tuple, since this doesn’t change the tuple.
You can also use the in operator to check if an element exists in the tuple.
Tuples are faster than lists. If you’re defining a constant set of values and all you’re ever going to do with it is iterate through it, use a tuple instead of a list.
It makes your code safer if you “write-protect” data that does not need to be changed.
count() Returns the number of times a specified value occurs in a tuple
index() Searches the tuple for a specified value and returns the position of where it was found


Dictionaries unordered
-Each key in a single Dictionary object must be unique.
-Dictionaries can also be used to sort, iterate and compare data.
-dict.get() to ensure key exists or use key in dict. example dict.get(key)
-Adding new items to a Dictionary: To add a value to a Dictionary, specify the new key and set a value. 
-Removing items from a Dictionary: del dict[key]: To remove a value from a dictionary, use the del method and specify the key to remove
-Counting Items in the Dictionary: Use the len() property to obtain a count of the number of key:value pairs in the dictionary.
-Use the variable name and the key value in brackets [] to get the value associated with the key.
-The .keys() and .values() methods return an array containing all the keys or values from the dictionary. 
- .items() to loop dictionary
-sorting dictionaries:
by key:
for k, v in sorted(room_num.items(),reverse=True):
    print(k + ' is in room ' + str(v))
    
 or 
 dict={}
for i,j in sorted(room_num.items()):
    dict[i]=j
 by values:
 print(sorted(room_num.values()))
 by keys backwards:
 for k, v in sorted(room_num.items(),reverse=True):
    print(k + ' is in room ' + str(v))  
    
another way to create dictionary from a list of tuples:
room_num1 = dict([('john', 425), ('tom', 212), ('sally', 325)])

NOT gives opposite

== Equal to

can share jupyter notebook as html file also. file->download as->html

additional:
Rational numbers in most programming languages are called Floating numbers - due to various factors, they are an approximation of their equivalent in Decimal.
In Javascript and Python the final result of 0.1 + 0.2 equals to 0.30000000000000004.
In C is 0.300000011920928955
0.1 doesn't exist in the 64-bit floating point world. Floating point can only represent (without loss of accuracy) the dyadic rationals, meaning rational numbers (can be divided by 2 integers) whose denominators are powers of 2. 
This means that 1/10 (or 0.1) is converted to a dyadic number that's just very close to 0.1. 
The error is tiny: for 64-bit floating point, it's a relative error around 10^-17
Floating point operations are fast and usually very accurate, but it's an approximate system. 
Comparison is fine, but equality checking is rarely what you want, because two numbers that are mathematically equal but generated by different processes-- e.g. (a + b) + c and a + (b + c)-- can be unequal.
Most languages do have fixed-point decimal classes in which 0.1 + 0.2 == 0.3. 
These avoid the problem for base-10, but not in general. You can step up to rational numbers and have perfect equality over those, but you have increasing space needs as the integers (numerator and denominator) get bigger over time. 
Floating-point and fixed-point arithmetic both throw away the least important digits at each iteration, but rational arithmetic throws nothing away with addition, subtraction, multiplication or division. (Of course, if you do a sqrt or exp or sin, you'll exit the rationals.)
The problem with rational-number arithmetic is that your space needs can grow linearly with the amount of computation that you do, which can hurt you on iterative algorithms.
Usually, the well-understood and typically negligible loss of precision that comes with floating point is preferred. 
It's better to know that your algorithm will perform in constant space and be very accurate, than to have one that can run out of memory in search of perfect accuracy... especially when perfect accuracy never exists in the real world.

If you want to round the number because that's what your mathematical formula or algorithm requires, then you want to use round. 
If you just want to restrict the display to a certain precision, then don't even use round and just format it as that string.
s for string, d to display decimal integers (10-base), and f which we’ll use to display floats with decimal places
eg print("${:.2f}".format(refund))
using formatters to increase field size and modify alignment can make your output more readable.
eg for i in range(3,13):
    print("{:6d} {:6d} {:6d}".format(i, i*i, i*i*i))
    
Backslashes allow you to have \"quotes\" inside your quotes!
Sometimes, more than one statement may be put on a single line. In Python a semicolon (;) can be used to separate multiple statements on the same line. 

loop thru item in lists: for i in list 
loop thru range: for i in range(100)

for LISTS, can ENUMERATE:
for index,value in enumerate(names):
    print(f"index {value}")
    
    
 for tuple in lists:
 animals = [
    ('zebra', 'mammal'),
    ('tuna', 'fish')
]

for animal, cls in animals:
    print(f"{animal} is a type of {cls}.")
    
for DICTIONARY, can use ITEMS:
 animals = {
    'zebra': 'mammal',
    'tuna': 'fish'
}

for animal, cls in animals.items():
    print(f"{animal} is a type of {cls}.")
    
 can get same result using just keys:
 for keys in animals.keys():
    print(f"{keys} is a type of {animals[keys]}.")
    
 generate random number:
 import random
 random.random()
 
 error handling:
 try:
 except:
 
 set and dictionary are unordered
 
 git stash: to commit
 git pull
 
 =* cannot work
 only *= works
 same for =+ dont work. only += works
 
 datatype: numbers,string,set,list,tuple,dictionary,boolean
 
 python have no termination character/ end of the line character. most of the time, enter will do
 
 You cannot split a statement into multiple lines in Python by pressing Enter. Instead, use the backslash (\) to indicate that a statement is continued on the next line.

dict.update() : is append not update if key is not same

input(""): lets ppl input and write. Input is stored as STRING

python is case sensitive

Values are assigned to variables creating an expression as follows: the variable is on the left side of the expression and the value you want to assign to the variable is on the right. 
You declare multiple variables by separating each variable name with a comma. 
For example:
a, b = True, False

 A local variable can only be accessed within the function it was declared in.
 
 Python sets the variable type based on the value that is assigned to it. 
 Unlike other languages, Python will change the variable type if the variable value is set to another value. 
 Most of the time Python will do variable conversion automatically.
 
 Python uses single quotes ' double quotes " and triple quotes """ to denote literal strings.
Only the triple quoted strings """ also will automatically continue for multiple lines.

Syntax is the set of rules that define what the various combinations of symbols mean. This tells the computer how to read the code. 

special syntax to sometimes make code more readable : print("The item {} is repeated {} times".format(var2,y))
y and var2 becomes string when printed
The {} are placeholders that are substituted by the variables element and count in the final string. 
another eg:
def titanic_name(first_name, last_name, title='Mr'):
    #print(last_name + ", " + title+". " + first_name)
    # printing is merely for you the developer to debug your code. In order to use result from titanic_name elsewhere in our code, we need to explicitly return it
    return '{}, {}. {}'.format(last_name,title,first_name)
    
Operators
arithmetic operators, comparison operators, concatenation operators, and logical operators.
When expressions contain operators from more than one category, arithmetic operators are evaluated first, comparison operators are evaluated next, and logical operators are evaluated last.
concatenate = combine 

Arithmetic operators are have order of precedence
-Exponentiation	**
-Unary negation	-
-Multiplication	*, -Division	/
-Modulus arithmetic	%
-Addition	+, -Subtraction	-
-String concatenation	& ( not an arithmetic operator, but in precedence it falls after all arithmetic operators and before all comparison operators.)

Comparison operators all have equal precedence; that is, they are evaluated in the left-to-right order in which they appear.
-Less than	<
-Greater than	>
-Less than or equal to	<=
-Greater than or equal to	>=
-Equality	==
-Inequality	!=
-Object equivalence	is

x = y # Sets x equal to y. this is assignment
x == y # Evaluates whether x is equal to y. this is comparison operator

logical operator
-Logical negation	not
-Logical conjunction	and
-Logical disjunction	or
-Logical exclusion	xor
-Logical equivalence	eqv
-Logical implication	imp

CONTROL FLOW: when we want to implement logic. eg conditionals, for loops, list comprehension, while loop, nested loops
There are three control flow statements in Python - if , for and while .
control flow (or flow of control) is the order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. 

conditional statements:
if…elif…elif…else nested statement
Python programming language assumes any non-zero and non-null values as TRUE, and if it is either zero or null, then it is assumed as FALSE value.

for loop statements
You can exit any for statement before the counter reaches its end value by using the break statement. 
Because you usually want to exit only in certain situations, such as when an error occurs, you could also use the if statement in the True statement block. If the condition is _False_, the loop runs as usual.

example of while loop:
while True:
    n = input("Please enter 'hello':")
    if n.strip() == 'hello':
        break

2 'for' in list comprehension: [(x, y) for x in [1,2,3] for y in [3,1,4] if x != y]
nested list comprehension example: [ y for x in non_flat if len(x) > 2 for y in x ]
---------------------------------------------WEEK 1 DAY 2 -------------------------------------------------------------
--------------------------------------------TUESDAY 7 APRIL -----------------------------------------------------------
----------------------------------- FUNCTIONS AND LIST COMPREHENSION---------------------------------------------------
Default value: Only if no parameter passed then default used.
Cannot put Default value for argument in first argument. doesn't seem to work

Function only reads variable defined inside

define function = function declaration

changing global variable inside a function:
g_var2 = "Outside"
def function3():
    global g_var2 # explicitly declare that the scope of this variable is global
    g_var2 = "Inside"

Split method by default look for a space. Doesn’t split when no space

lambda expression==anonymous function: no need name for function
can let variable_name = lambda expression
eg g = lambda x1, x2: x1+x2
eg
f=lambda name: 'Howdy, {}'.format(name)
f('John')

lambda expression can have no input also
eg lambda: "why like that"

to sort names using lambda expression:
authors.sort(key=lambda name:name.split(" ").lower())

#lamba expression in function:
def quadratic_function(a,b,c):
    return lambda x: a*x**2+b*x+c

quadratic_function(3,0,1)(2)

repeat without counting: for _ in range(10)

list comprehension example:
# Recreate squared_numbers using list comprehension
numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
squared_numbers = [num**2 for num in numbers] #in lists

list comprehension with filtering example:
classroom_teachers=[members for members in classroom if members['role'] == "teacher"]

list comprehension can work for dictionaries also. wrap in {} instead of []
list comprehension with dictonary example:
users={i: v.lower() for i,v in users.items()}
Or
users={i:users[i].lower() for i in users}

#create list of integers
list(range(1,101))

additional:
map(f,data) is running iterable of function over data
has no datatype. to return list: list(map(f,data))
use lambda expression to iterate if its just a single expression not function

filter(function/lambda expression,data): this filters only return for which function is true
filter(None,data): need to be super careful. None in python represents "",0,0.0,[],{},() etc. so it can filter away zero values also

reduce function now not built in. its under functools
it runs iteration against its previous solution.
reduce(f,data)

---------------------------------------------WEEK 1 DAY 3 -------------------------------------------------------------
--------------------------------------------WEDNESDAY 8 APRIL -----------------------------------------------------
Don’t append or remove the main list while trying to run a for loop. Will cause the length of the list to change

Experiment: procedure can be repeat inifite number of times and has well defined set of outcomes
Sample space is all outcomes of the experiment
Event: collection of outcome of experiment: Eg. A = Event of getting 2 heads

max(["words","words]): max list of words give the word in the lowest end of alphabetical order. not length
should do this:  len(max(list_of_words, key=len))

list.remove(): remove a specific value

what is datascience process?
-define problem
-gather data
-explore data
-modelling with data
-evaluate model
-solve business problem
order may not be as such. more explore first then know what problem to fix

know when will the model work and will not.
---------------------------------------------WEEK 1 DAY 4 -------------------------------------------------------------
--------------------------------------------THURSDAY 9 APRIL -----------------------------------------------------
Distribution: frequency of all possible outcomes

Distribution illustration and formula: https://seeing-theory.brown.edu/probability-distributions/index.html#section2

Probability mass function shows us the probability that our random variable takes on a specific value.

Cumulative distribution function shows us the probability that our random variable takes on any value less than or equal to a specific value.

---------------------------------------------WEEK 2 DAY 1 -------------------------------------------------------------
--------------------------------------------MONDAY 13 APRIL -----------------------------------------------------
--------------------------------------------INTRO TO PANDAS------------------------------------------------------
methods:
.shape
.columns
.read_csv() : Can read a specific number of rows using nrows=
.describe()
.info()
.head()
.tail()
.value_counts() : applied to columns, will count number of each categorical variable
.mean()
.median()
.quantile()
.to_datetime() : to convert to datatime data type
.isin( )
.between( , )
.sort_values()
.sort_values([ , ]) : on 2 columns
np.mean()
np.log()
.groupyby : eg df.groupby('var1')['var2'].agg(['count', 'mean', function])
.drop( ,axis=1,inplace=True) : axis=0 is rows. axis=1 is column. inplace=True replace original data. otherwise original df not affected
.columns.str.lower() :  eg df.columns.str.lower().  : lowercase headers. do note data not replaced
.str.lower() :  eg df['var'].str.lower() :l owercase headers. do note data not replaced
.rename() :  eg df.rename(columns={"colors reported": "colors", "shape reported":"shape"},inplace=True).  : rename some
.columns  : eg df.columns = ['city','colors','shape','state','time'].  : rename all
.str.replace(,) :  eg df['var'].str.replace('o', 'bro').  : replace values in df
.dt.year :  eg df['time'].dt.year.  : take out year from datatime data type
.isnull()
.notnull()
.loc : eg df.loc[df['var1'].notnull(),:] : filter out missing values
.to csv() : export data. eg. df.loc[df['var']>30,:].to_csv('datasets/highmpgcars.csv',index=False)
.merge(,,) :  eg pd.merge(df1, df2, how='left')  : merge to the main dataframe1
.crosstab(,) :  eg pd.crosstab(df['var1'], df['var2'])
.Categorical(,) : eg pd.Categorical(df['var1'], categories=['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']) : reorder categories
.map() :  eg df['var2'] = df['var1'].map({4: 'Four', 6: 'Six', 8: 'Eight'}) or df['var2'] = df['var1'].map(function) or df['var2'] = df['var1'].map(lambda x: "Efficient" if x>20 else "Wasteful")
.apply(function, axis=1). : axis=0 is rows. axis=1 is column. 
.map

Others:
plt.figure(figsize=( , ))
plt.hist( , bins=)
np.array([ , , , ,...])
v[v<40]:return values that fulfil condition
pd.Series([ , , ..])  : creata panda series. not homogenous
plt.subplot(,,) : decide number of plots will be drawn
sorted(list) returns a list. Capital letter sorted first.
list.sort() don’t return a list. It changes the list

dataframe (panda object. data container):
df[ ] or df.'' : one column
df[ ][ ]: one cell
df[' ']<40
df[(df[' ']==4) | (df[' ']==1)]
~ df[ ]: not
df[[' ']] into dataframe. avoid just df[' ']
loc and iloc to filter rows and columns both:
df.loc[(df[' ']==4) | (df[' ']==1), [ ,  ,  ,  ...]]
df.iloc[ , ]

panda gives ordinal instead of simply categorical data
map is for dataseries. apply for dataframe. map and apply same if apply for dataseries

dict-in-dict into dataframe: pd.DataFrame(dict).T

---------------------------------------------WEEK 2 DAY 2 -------------------------------------------------------------
--------------------------------------------TUESDAY 14 APRIL -----------------------------------------------------
--------------------------------------------DATA VISUALISATION------------------------------------------------------
method:
.dtypes
.index
.sort_values
.unique #panda unique. or np.unique from numpy
.sort_index
.plot(kind='') #kind=bar/barh/hist/box/scatter
sns.pairplot() #same as the one below
pd.plotting.scatter_matrix(df[ , ])
sns.distplot()
sns.heatmap(df.corr)
plt.style.use() #stylesheet
random.sample: is by python. no replace
np.random.choice: is by np. got replace

others:
-for boxplot, "min" line is Q1-1.5*IQR, "max" line is Q3+1.5*IQR
-text viz is also good to emphasise just one value
-choose visual that bring across the message
-have slides with just graph and no text describing them. explain the graph instead
-ideally use [] to refer to df columns instead of

---------------------------------------------WEEK 2 DAY 3 -------------------------------------------------------------
--------------------------------------------WEDNESDAY 15 APRIL -----------------------------------------------------
-----------------------------------------EXPLORATORY DATA ANALYSIS--------------------------------------------------
methods:
.copy : to copy to a new variable instead of pointing to the same variable
.dropna()
.astype()
sns.boxplot
.dtypes()
plt.figure()
.isnull().sum()

others:
.() : is like a method whereas . is like an attribute

---------------------------------------------WEEK 2 DAY 4 -------------------------------------------------------------
--------------------------------------------THURSDAY 16 APRIL -----------------------------------------------------
--------------------------------CONFIDENCE INTERVAL AND HYPOTHESIS TESTING-------------------------------------------
methods:
.var : variance
.tolist
pd.get_dummies() : make dummies from values

to setup folder to push:
1) create repo on github
2) git init
3) git add .                            : . is to add all files. to add specific type filename.
4) git commit -m "comment"
5) git remote add origin https://...
6) git remote -v                        : is to check if file pointing correct
7) git push -u origin master

others:
-np.NaN : null value
-masking: df[(...==...) | (...==...)]
-H0 is about the population. We draw sample to make inference about the population.
-standard deviation of sampling distribution = standard error

---------------------------------------------WEEK 2 DAY 5 -------------------------------------------------------------
--------------------------------------------FRIDAY 17 APRIL -----------------------------------------------------
--------------------------------ADVANCED TRANSFORMATION USING PANDAS---------------------------------------------
df.values = array : df to array
df.sum() : give column. aggregation over entire dataset
  .mean() : aggregation over entire dataset
pd.melt(df: id_var=[ , ]). :put df into 2 column. id_var is those column you don't want to melt
melted.pivot():  melted=pd.melt(df)
.reset_index()

Others:
-for , in enumerate(): : to access array
- .map for series only. lambda takes each values

Groupby:
df.groupby().describe()
            .sum() :aggregation with group + pandas. aggregation over sepcific variables
            .mean() :aggregation with group + pandas. aggregation over sepcific variables
            .agg({variable:[np.mean,np.median]}). :aggregation with group + pandas. aggregation over sepcific variables
            .count() : if any group specifically high, may not be good to use this variable
            
After Groupby:
group.get_group("groupname") : get subset of dataframe. access each group. 
.groups : shows groups in groupby by index.
for groupname,groupdf in category: : iterate over the groups

---------------------------------------------WEEK 3 DAY 2 -------------------------------------------------------------
--------------------------------------------TUESDAY 21 APRIL -----------------------------------------------------
--------------------------------------------LINEAR REGRESSION-----------------------------------------------------
### Plot scatter
# create a figure
fig = plt.figure(figsize=(12,7))
# get current axis of that figure
ax = fig.gca()
# plot a scatter plot on it with our data
ax.scatter(x,y);
#plot line of fit
ax.plot(df['X'], df['Linear_Yhat'], color='k');
#plot baseline 
ax.plot((df['X'].min(),df['X'].max()),(np.mean(df['Y']),np.mean(df['Y'])),color='r');
# iterate over predictions
for _, row in df.iterrows():
    plt.plot((row['X'], row['X']), (row['Y'], row['Linear_Yhat']), 'r-')
    
Methods:
np.std(df['X'],ddof=1) : get sd
r_xy = df.corr().loc['X','Y']   or np.corrcoef(df['X'],df['Y'])[1][0]. #get corr
df.loc[:,['var1','var2',..]] : get the columns you want
.values : get values from df
X.T : The transpose of a matrix is calculated by appending .T to the matrix
np.dot(mat1, mat2): Matrices multipled in the formula should be done with the "dot product"
np.linalg.inv(): Inverting a matrix is done using



Goal:
minimize loss function: where the "loss" here is the sum of squared residuals or or residual sum of squares (RSS) or SSE (sum of squared errors).
 RSS = sum{yi-yi_hat}^2  
 code: np.sum(np.square(df['Y'] - df['Mean_yhat']))
 equations for the intercept and slope that minimize the RSS is: 𝛽̂_0_hat= 𝑦_bar − (𝛽̂1_hat * x_bar)
                                                                 𝛽̂_1_hat= (𝑟_𝑋𝑌 * 𝑠𝑌 ) / 𝑠𝑋
  such that:
  𝑦_bar  : the sample mean of observed values  𝑌 
  𝑥_bar : the sample mean of observed values  𝑋 
  𝑠𝑌  : the sample standard deviation of observed values  𝑌 
  𝑠𝑋  : the sample standard deviation of observed values  𝑋 
  𝑟𝑋𝑌  : the sample Pearson correlation coefficient between observed  𝑋  and  𝑌
  
  for multiple linear regression to minimize sse: 𝛽 = (𝑋𝑇 * 𝑋)^-1 * 𝑋𝑇* 𝑦
  
  to get betas: 
  from sklearn.linear_model import LinearRegression
  linreg = LinearRegression()
  linreg.fit(X, price)
  print(linreg.coef_)

Common metrics for evaluating regression models:
- MSE
  np.mean(np.square(df['Y']-df['Mean_yhat']))
- root mean squared error (RMSE) is a standard measure of model performance. 
  It is the square root of the mean of the sum of squared residuals:
  RMSE= sqrt (1/𝑛 ∑𝑖(𝑦𝑖_hat−𝑦𝑖)^2)
 The smaller the root mean squared error, the better your model fits the data.
 code: from sklearn.metrics import mean_squared_error, r2_score
       print(np.sqrt(mean_squared_error(target, predictions))) 
-coefficient of determination  𝑅2 : 
 𝑅2=1−𝑆𝑆𝑟𝑒𝑔/𝑆𝑆𝑡𝑜𝑡 
Where the regression sum of squares is the sum of squared residuals for our model:
𝑆𝑆𝑟𝑒𝑔=∑𝑖(y_hat −𝑦i)2 
And the total sum of squares is the sum of squared residuals for the baseline model. This is essentially the variance of our target.
𝑆𝑆𝑡𝑜𝑡=∑𝑖(𝑦𝑖−𝑦_bar)2 
𝑅^2  is the most common metric to evaluate a regression and is the default scoring measure in sklearn. When we cover classification models, the .score function instead defaults to accuracy.
A negative  𝑅2  means that the regression model is performing worse than the baseline model. 

Others:
-residual 𝜀𝑖 = yi-yi_hat. residuals = y-predictions
-simple linear regression is an estimator of the expected value (mean) of 𝑌
-y_hat = 𝛽̂_0_hat + 𝛽̂_1_hat 𝑋
-predict with model: df['Linear_Yhat'] = beta_0 + beta_1 * df['X']

-The simple linear regression estimators described above have really nice properties:
They are unbiased. (a.k.a. the statistics will on average equal the true parameter value)
Among all unbiased estimators, the estimators above have the smallest possible variance.
They are consistent. (a.k.a. as your sample size n goes to infinity, the statistic converges to the true parameter value)
However, these are just mathematical properties. They do not take into account real-world situations. F
When building a model, make sure your model makes sense! You are responsible for the interpretation of the model. All the computer will do is optimize. 

-There are some critical assumptions involved in simple linear regression that you must be aware of:
Linearity: Y and X must have an approximately linear relationship.
Independence: Errors (residuals) 𝜀𝑖 and 𝜀𝑗 must be independent of one another for any 𝑖≠𝑗.
Normality: The errors (residuals) follow a Normal distribution with mean 0.
Equality of Variances (Homoscedasticity of errors): The errors (residuals) should have a roughly consistent pattern, regardless of the value of X. (There should be no discernable relationship between X and the residuals.)
The mnemonic LINE is a useful way to remember these four assumptions.
If all four assumptions are true, the following holds: 𝑌𝑖∼𝑁(𝛽0+𝛽1𝑋𝑖,𝜎)

-sklearn is the machine learning package
statsmodels is the statistics package
Though the terms have immense overlap, machine learning tends to be more prediction focused while statistics is more inference focused.

Steps for linear regression using sklearn:
1) #from sklearn import linear_model
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
2) # initiate model
model = LinearRegression()
3) # fit the model
X = df[['var1']]
y = target
model.fit(X,y)
predictions  =  model.predict(X)
4) score        =  model.score(X, y)  #a class method / function that returns the coefficient of determination R^2 of the prediction (for regression models).
# Plot the model
plt.figure(figsize=(12,7))
plt.scatter(predictions,y,c='r')
plt.xlabel('....')
plt.ylabel('....');
#get attributes and interpret them
model.coef_
model.intercept_
#evaluate:
from sklearn.metrics import mean_squared_error, r2_score
print(np.sqrt(mean_squared_error(target, predictions))) 
print(r2_score(target, predictions)) #model.score also can


fit MLR example:
lm = linear_model.LinearRegression()
# two predictors
X = df[['carat','table']].values
y = target
model = lm.fit(X, y)
predictions = model.predict(X)
score = model.score(X, y)
# Plot the model
plt.figure(figsize=(8,8))
plt.scatter(predictions, y, s=30, c='r', marker='+', zorder=10)
plt.xlabel("Predicted Values from CARAT + TABLE - $\hat{y}$")
plt.ylabel("Actual Values PRICE - y")
plt.show()
print("score: ", score)

Fitting a linear regression using statsmodels:
NOTE:  The statsmodels process is slightly different:
We manually make a new column for the intercept in our design matrix  𝑋 .
The  𝑦  target variable comes before the  𝑋  predictor
The data is provided during the instantiation of the model object, then fit is called without the data.
code:
import statsmodels.api as sm
X = df[[['carat','table']].values
# manually add the intercept column:
X = sm.add_constant(X)
y = target
model = sm.OLS(y, X)
model = model.fit()
predictions = model.predict()

---------------------------------------------WEEK 3 DAY 3 -------------------------------------------------------------
--------------------------------------------WEDNESDAY 22 APRIL -----------------------------------------------------
-----------------------------------BIAS-VARIANCE TRADEOFF & TRAIN-TEST-SPLIT----------------------------------------
deal with categorical data:
-one hot encoding: .get_dummies() : convert string to multiple columns binary
-ordinal : for ordered data eg good,very good. difference is 1unit. convert string to just 1 column values

supervised learning: with target y given
1) classification: target is categorical/discrete. x can be cts or discrete
2) regression: target is cts. x can be cts or discrete

bias = difference between prediction and true relationship between the variables
variance = ability to replicate the results with similar sum of squared errors for future datasets. high var dont perform well for data that has not seen.
can be caused by variables with high collinearity
how to tell overfit: var will increase with new dataset.
solutions: regularization, bagging and boosting

train-test-split if unbalanced proportion for target, rmbr to stratify

---------------------------------------------WEEK 3 DAY 4 -------------------------------------------------------------
--------------------------------------------THURSDAY 23 APRIL -----------------------------------------------------
------------------------------------FEATURE ENGINEERING AND REGULARIZATION-----------------------------------------
Others:
-features need to be standardized in regularized model
-Elastic net combines ridge and lasso penalties
-Ridge regression: increase lambda, target less sensitive to predictors
-Ridge is square of coefficient but lasso is absolute
-ridge can only make coefficient small but lasso can make coefficiemt zero (feature selection technique)
-overfit: dont generalize well,training the noise,low bias but high var

## select all the columns that are not the target
nc = [x for x in df.columns if x != target]

## Standardize to dataframe
df[nc] = (df[nc]-df[nc].mean())/ df[nc].std()

standardization only to x. no need to y.

---------------------------------------------WEEK 3 DAY 5 -------------------------------------------------------------
--------------------------------------------FRIDAY 23 APRIL -----------------------------------------------------
-------------------------------------------MISSING DATA IMPUTATION-----------------------------------------------
correct term: data science with missing data not 'handle missing data'

3 types of missing data:
-Missing completely at random (MCAR)
-Missing at random (MAR)
-Not missing at random (NMAR)

Within the 3 types:
-item nonresponse: some values not observed
-unit nonresponse: a value not observed

3 methods to solve for missing data:
-avoid it
-ignore it
-accoount for it

to avoid unit nonresponse:
-change data collection method
-avoid burden respondent
-improve accessibility
-change timing of survey

to account for unit nonresponse:
-weight class distribution. reweight all respondents by true proportion/proportion of response

avoid item nonresponse:
-minimize length of questionnaire
-design questionnaire with respondent in mind

ignore item nonresponse:
-complete case analysis: drops obs with any missing value. cons: info loss
-avaialble case analysis: drops no obs calculate results based on available data. cons: results may be abnormal. egg corr >1

account for item nonresponse:
-deductive imputation
-inferential imputation (fill na with distribution): mean/median/model, regression, stocashtic (MAR data), hot-deck or proper imputation
-pattern submodel approach

to deduce MCAR,MAR or NMAR:
-Little'test for MCAR vs MAR. no empirical test for NMAR
-partition data for missing and non-missing and compare
-think about why data is missing and how it affects the study.

always think about data quality

if little data missing: fill them
if many, drop them
if some data missing: common approach proper imputation, stochastic imputation, pattern submodel

to interpret categorical or integer: boxplot or compare the means if drastically different. use groupby

Many missing data approaches simplify the problem by throwing away data. We discuss in this section how these approaches may lead to biased estimates (one of
these methods tries to directly address this issue). In addition, throwing away data can lead to estimates with larger standard errors due to reduced sample size.
http://www.stat.columbia.edu/~gelman/arm/missing.pdf

---------------------------------------------WEEK 4 DAY 1 -------------------------------------------------------------
--------------------------------------------MONDAY 27 APRIL -----------------------------------------------------
---------------------------CLASSIFICATION, LOGISTIC REGRSSION, K-NEAREST NEIGHBORS-------------------------------
unsupervised learning: no target y. 
supervised learning:
1) regression: uncountable number of outcomes y
2) classfication: countable number of outcomes y. binary/multiclass classification. eg logisitic regression

k-nearest neighbors:
-non parametric model: no coefficients for the different predictors and our estimate is not represented by a formula of our predictor variables
                       and no assumptions about the distribution for our data
                       
                       
hyper parameters: parameters you choose for your model. eg for KNN is n-neighbors

logistic regression for multiclass is one vs all but there are ways for it to give probability like knn
knn vs logisitc regression: logisitic regression is parametric model. can do inference on the predictors influence on target. knn is more for prediction

Assumptions for logistic regression:
 Linearity: The independent variables X1...Xm are linearly related to the logit of the probability
that Y = 1 or, equivalently, the log-odds that Y = 1.
 Independence of Errors: The observations y1...yn are independent of one another.
 Distribution of Errors: Each observation yi follows a Bernoulli distribution with probability of
success pi.
 Independence of Independent Variables: The independent variables X1...Xm are independent
of one another.

---------------------------------------------WEEK 4 DAY 2 -------------------------------------------------------------
--------------------------------------------TUESDAY 28 APRIL -----------------------------------------------------
------------------------------------------CLASSIFICATION METRICS--------------------------------------------------
-Use training data to imputate missing data for test data
-remember to ensure new features values in testing is also in training data in order for the model to understand. use techniques like intersection or what to check
-besides lasso selection, got forward stepwise selection or backward stepwise selection also
-for categorical, its either correct fit or wrong. so use rmse etc wont work. 
accuracy = all correct / (all predictions) = TP+TN / (TP+TN+FP+FN)
misclassfication = 1- accuracy. 
sensitivity or True positive rate or recall = TP/ (all positives) = TP/ (TP+FN)
specificity or True negative rate= TN/ (all negatives) = TN /(TN+FP)
precision or positive predictive value= TP/ predicted positive = TP/ (TP+FP)
to calculate, make confusion matrix of actual positive and actual negative, and predicted positive and predicted negative
F1 score = predicted+recall

TP: predicted fraud and is actual fraud (correct)
FP: predicted fraud but not actually fraud
FN: predicted ont fraud but actually is fraud
TN: predicted not fraud and its not fraud (correct)

type 1 error: FP
type 2 error: FN

variance can look at cross_val_score
bias can look at r2 or accuracy from score. from score also see if got drop. if yes, suggest overfit. do knn.score(X_train_scaled,y_train)  and knn.score(X_test_scaled,y_test) 

---------------------------------------------WEEK 4 DAY 3 -------------------------------------------------------------
--------------------------------------------WEDNESDAY 29 APRIL -----------------------------------------------------
--------------------------------------HYPERPARAMETER TUNING AND PIPELINES-------------------------------------------
Logistic=logit_inv
%%time: show time computation

sklearn has two types of classes: estimators and transformers.
Estimators are essentially models.
Estimators have a fit and predict method.
Transformers are not models. They transform your data using similar syntax to estimators. 
Instead of fit and predict, they have fit and transform methods. In fact, since you fit and transform together so often, they have a shortcut: .fit_transform
We've seen a few transformers, including StandardScaler() and PolynomialFeatures(). There's also OneHotEncoder() for dummy encoding and LabelEncoder() for factorizing variables. Later we'll see PCA(), which is also a transformer.
Transformers may have hyperparameters as well - but we can't GridSearch over a transformer! There's no way to get an accuracy (or other) score from just a transformer, since a transformer can't predict!

Statistical parameters are quantities that a model can learn or estimate. Examples include  𝛽0  and  𝛽1  in a linear model.
Hyperparameters are quantities our model cannot learn, but affect the fit of our model. Examples include  𝑘  in  𝑘 -nearest neighbors and  𝑎𝑙𝑝ℎ𝑎  in regularization.
---------------------------------------------WEEK 4 DAY 4 -------------------------------------------------------------
---------------------------------------------THURSDAY 30 APRIL -----------------------------------------------------
----------------------------------------------------API-------------------------------------------------------------
curl www.example.com in terminal to open url

json look like python dictionary 
json file inside json validator (can google). it will convert to nicer dictionary-like format

read api documentation before calling to know how to call properly
---------------------------------------------WEEK 5 DAY 1 -------------------------------------------------------------
---------------------------------------------MONDAY 4 MAY-----------------------------------------------------
-----------------------------------------------PROJECT 2------------------------------------------------------
kaggle is unique where test file given with no target.
since train file enough data, can do train-validation split on the train.csv and perform workflow as per normal
option 2 is to run workflow on the entire train data set. ideally just do train-val split.

ending comparing .score is compare for train vs valid set to see if model got overfit.
can use pipeline.score so no need do standardscaler separately

cross_val_score is do only for training set before .score to get estimate.
cross_val_score internally got do train_test_split. pipeline_gridsearch(X_train,y_train).score = cv(X,y)
can just cross_val_score(pipeline,train)

gridsearch is to choose hyperparameter. do cross_val_score after that. then final score.

parameter model can got calculation. fit is for X_train. wont affect is send X_test to pipeline cause once fit, it will only transform for scaler

---------------------------------------------WEEK 5 DAY 2 -------------------------------------------------------------
--------------------------------------------TUESDAY 5 MAY-----------------------------------------------------
--------------------------------------WEB SCRAPING & REGEX----------------------------------------------------
# To install libraries
# Beautiful Soup:
> conda install beautifulsoup4
> conda install lxml

# Or if conda doesn't work:
> pip install beautifulsoup4
> pip install lxml

beautiful soup and xpath do same thing. just automation usually use xpath

regular expression : search large data to find pattern. filter out data in certain way
https://regex101.com/ to practice

usually use re.search() to get regex correct before applying to re.findall

Selenium also another automation scrapping tool like scrapy

scrapy:
1)  Create a new Scrapy project. follow step 1 in notes
2) in item.py file, follow step 2 in notes
3) Let's write our first file, called craigslist_spider.py using sumblime, and put it in our /spiders directory.
'''
import scrapy
from craigslist.items import CraigslistItem
from scrapy.selector import Selector

class CraigslistSpider(scrapy.Spider):
    name = "craigslist"
    allowed_domains = ["craigslist.org"]
    start_urls = [
        "https://atlanta.craigslist.org/search/cto"
    ]
    
def parse(self, response): # Define parse() function. 
    items = [] # Element for storing scraped information.
    hxs = Selector(response) # Selector allows us to grab HTML from the response (target website).
    for sel in hxs.xpath("//li[@class='result-row']/p"): # Because we're using XPath language, we need to specify that the paragraphs we're trying to isolate are expressed via XPath.
        item = CraigslistItem()
        item['title'] =  sel.xpath("a/text()").extract() # Title text from the 'a' element. 
        item['link']  =  sel.xpath("a/@href").extract() # Href/URL from the 'a' element. 
        item['price'] =  sel.xpath('span/span[@class="result-price"]/text()').extract()[0]
                # Price from the result price class nested in a few span elements.
        items.append(item)
    return items # Shows scraped information as terminal output.
 '''
 put the above in the file. remember check the spacing
 
 4) run this in terminal at the craigslist/craigslist directory to generate the csv file "item.csv": 
    scrapy crawl craigslist -o items.csv -t csv

good auto webscape tool:Edgar web scraping
---------------------------------------------WEEK 5 DAY 3 -------------------------------------------------------------
--------------------------------------------WEDNESDAY 6 MAY-----------------------------------------------------
------------------------------------------NATURAL LANGUAGE PROCESSING-----------------------------------------------------
Pre-Processing:
-Tokenizing: chop words using regex (split manually also can. in nlp 1)
-Regular Expressions: regex
-Lemmatizing/Stemming: converte to root word. stemming more crude than lemmatizing
-Cleaning (i.e. removing HTML) (use beautiful soup)

NLP convert unstructed text data to structured text data. 
Sentiment analysis is an area of natural language processing in which we seek to classify text as having positive or negative emotion.

In Python, searching a set is much faster than searching a list, so convert the stopwords to a set.

convert text to numerical: countvectorizer
(can add things like len of review etc not just countvectorizer method to complement in the model)
CountVectorizer requires a vector, so make sure you set X to be a pandas Series, not a DataFrame

bag of words: transformation tool such as countvectorizer or tfidvectorizer. convert text to data

A collection of text is a document. You can think of a document as a row in your feature matrix.
A collection of documents is a corpus. You can think of your full dataframe as the corpus.

nltk vs sklearn stopwords are different. 

dont want number of features to be more than number of obs. hence usually set max_features

A pipeline stacks together one or more transformers with an estimator at the end. The estimator allows us to .predict() and get a score!

if set stopwords, words like not good will not be taken for ngram

can use countvectorizer hyperparameter tokenizer/stopwords etc but better to split it up to see each step for backtracking purpose
---------------------------------------------WEEK 5 DAY 5 -------------------------------------------------------------
--------------------------------------------FRIDAY 8 MAY-----------------------------------------------------------
-------------------------------------SQL(brief) & NAIVE BAYES------------------------------------------------------
RDBMS = Relational database management system

SQL = structured query language = english-like statements to retrieve data usually
each row is a record. each key identifies the record

To build a web site that shows data from a database, you will need:
An RDBMS database program (i.e. MS Access, SQL Server, MySQL)
To use a server-side scripting language, like PHP or ASP
To use SQL to get the data you want
To use HTML / CSS to style the page

bayes' theorem:
P(A|B)P(B)=P(B|A)P(A)

naive bayes algorithm:
-is a classification modeling technique
-relies on bayes theorem
-assumes features are independent of one another!!
-cons: while classification is usually accurate, predict probability is usually quite bad. (.predict is ok but not .predict_proba_)

naive bayes model to use:
-bernoulliNB: feature set X are binary
-multinomialNB: feature set X are integers. can use for countvectorizer data or tfidfvectorizer data
-gaussianNB: feature set X is normally distributed (Practically, though, it gets used whenever BernoulliNB and MultinomialNB are inappropriate.)

need to decide prior also. can be set.

naive bayes also a parametric model. coefficient of nb model is useful

install spacy library:
Pip install -U spacy
python3 -m spacy download en_core_web_sm

-----------------------------------------------WEEK 6 DAY 1 ---------------------------------------------------------
---------------------------------------------MONDAY 11 MAY-----------------------------------------------------------
------------------------------------------CART, BOOTSTRAPPING AND BAGGING -------------------------------------------
multiclass != multilabel
use threshold to decide if fall into a few label using existing prob

decision tree is easier to visualize. if-else type of intuition. split data until target y is 'pure' (same)
decision tree split based on highest drop in gini/rsme
uses gini impurity to measure impurity for decision tree
gini = 0 means pure
complexity alpha is like alpha in regularization. default is 0.
decision tree cons: tend to learn irregular pattern, cause overfit training data easily
only concern for accuracy on test data. better test score will mean drop in train score in decision tree models.
Decision trees are nonparametric, meaning we don't make assumptions about how our data or errors are distributed.

Where do decision trees tend to fall on the Bias/Variance spectrum?
Decision trees very easily overfit.
]They tend to suffer from high error due to variance.

What is bootstrapping?
bootstrapping is random resampling with replacement. a way of sampling data: use one sample to create more samples.
Take many sub-samples (say 𝐵) of size 𝑛 from your sample with replacement. These are called bootstrapped samples.
You have now generated 𝐵 bootstrapped samples, where each sample is of size 𝑛
Instead of building one model on our original sample, we will now build one model on each bootstrapped sample, giving us  𝐵  models in total!
This sets up the idea of an ensemble model.
bootstrapped sample should use same n as X_train (not full data set. should still leave validation set out. n refers to number of X_train obs) so it's comparable between the bootstrapped samples and original sample data. 
bootstrap use on training data.normally after train-test-split.
We bootstrap when fitting bagged decision trees so that we can fit multiple decision trees on slightly different sets of data. Bagged decision trees tend to outperform single decision trees.
Bootstrapping can also be used to conduct hypothesis tests and generate confidence intervals (use calculation or via bootstrap) directly from resampled data.
It might be impossible to develop one model that globally optimizes our objective function. (Remember that CART reach locally-optimal solutions that aren't guaranteed to be the globally-optimal solution.) In these cases, it may be impossible for one CART to arrive at the true function.
However, generating many different models and averaging their predictions may allow us to get results that are closer to the global optimum than any individual model.

bagging = bootstrap + model + aggregating = bootstrap aggregation
default bagging classifier from sklearn uses decision tree

---------------------------------------------WEEK 6 DAY 2 -------------------------------------------------------------
-------------------------------------------TUESDAY 12 MAY-----------------------------------------------------------
----------------------------------------RANDOM TREE, EXTRATREES AND BOOSTING----------------------------------------
With bagged decision trees, we generate many different trees on pretty similar data. 
These trees are strongly correlated with one another. Because these trees are correlated with one another, they will have high variance. 

If  𝑇1  and  𝑇2  are highly correlated, then the variance will about as high as we'd see with individual decision trees. 
By "de-correlating" our trees from one another, we can drastically reduce the variance of our model.

Random forests differ from bagging decision trees in only one way: 
they use a modified tree learning algorithm that selects, at each split in the learning process, a random subset of the features. 
This process is sometimes called the random subspace method.
still using bootstrapping

random forest: every tree at every note use random subset of features
useful when we have very strong predictors that will keep being used in decision tree despite bootstrapping (using different data)

For a problem with 𝑝 features, it is typical to use:
√p  (rounded down) features in each split for a classification problem.
𝑝/3  (rounded down) with a minimum node size of 5 as the default for a regression problem.

extratrees: not only use random subset of eatures, use random of split. still using bootstrapped data
 
 Random forest: max_features = 'None' = bagged decision tree. also need set n_estimator=1 because rf is ensemble technique

bagging focus on reducing variance
boosting focus on reducing bias. also ensemble technique

The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as a single-split tree) on repeatedly modified versions of the data (changing prob of picking the data). 
After each fit, the importance weights on each observation need to be updated.

votingclassifier is a voting system. different model predict and then vote

Gradient boosting Regressors look at residuals. Is like regression makes prediction not classification

Can built say boosting on top of decision tree etc? yes. But harder to interpret.
Voting more common than stacked model after another and harder to interpret.

All model do bootstrap first including boosting. By default already run bootstrap

Look at both train n test so they roughly same. not just push test. Decision all these tend to overfit due to the way it works. 
So just need tune further to fit a better fit for both train and test.
---------------------------------------------WEEK 6 DAY 3 -------------------------------------------------------------
-------------------------------------------WEDNESDAY 13 MAY-----------------------------------------------------------
----------------------------------------SUPPORT VECTOR MACHINE--------------------------------------------------------
svm is a classification model different from decision tree

specific term used to describe how SVMs transform features into higher dimensions is called the kernel trick

cannot use maximum margin classifier if have overlaping class

linerly separated can use sv classifier.

if not use svm

SVMs will have two main hyperparameters: C and kernel.

svm can use for multiclass classification also
---------------------------------------------WEEK 6 DAY 4 -------------------------------------------------------------
-------------------------------------------THURSDAY 14 MAY-----------------------------------------------------------
----------------------------------------GENERALIZED LINEAR MODEL-----------------------------------------------------
glm is parametric model (important for inference)

a linear model is can be written in function form. even polynomial

prediction vs inference: convern y_hat vs beta_hat

why logistic regression (classification, but also GLM) no error term? bernoulli distribution component
y distributed as bernoulli.
yi~ Ber(p_i)
logit p_i = X_i T (beta)

OLS: (regression. also GLM)
yi~ N(miu_i,sigma)
miu_i = X_i T (beta)

GLM:
-systematic component(linear component)
-random component
-link function (identity function): maps miu to real numbers. eg for logisitc reg is logit function or log odds.

python is Object-Oriented Programming. not functional programming.
The OOP paradigm involves creating your own data types. 
These data types can maintain their own state and have their own methods and functions assocated with them.
---------------------------------------------WEEK 6 DAY 5 -------------------------------------------------------------
-------------------------------------------FRIDAY 15 MAY-----------------------------------------------------------
---------------------------------------------API & FLASK-----------------------------------------------------------
An endpoint is a URL pattern used to communicate with an API.

run virtual env:

mkdir ~/virtualenvs
 cd ~/virtualenvs
virtualenv my_special_env
cd my_special_env
. bin/activate : must have . Space


Once activated your can proceed to pip install all relevant libraries you need. Do note that since this is a new (virtual) environment, previous installations of other libraries in base library do not exist. So you have to install it again.
pip install Flask pandas scikit-learn scipy

To deactivate: deactivate

export FLASK_APP=service.py
export FLASK_DEBUG=1 ## check for debug
flask run

when edit py file, no need refresh to open. just use the same url

http://127.0.0.1:5000/predict-student?speed=15 : send speed=15 to the endpoint

---------------------------------------------WEEK 7 DAY 1 -------------------------------------------------------------
--------------------------------------------MONDAY 18 MAY------------------------------------------------------------
------------------------------------------P3 AND INTRO TO AWS AND DOCKERS---------------------------------------------
Feedback for P3 from instructor for all teams:
dont assume people know what is scatterplot
Talk about the data -> what are the kind of discussions that happen in both the subreddits
EDA
Wordcloud -> unigram, bigram etc
Length
Upvotes
Comments
Time of post etc.
Add to your stop words
User activity -> rate of activity
Which class has a greater effect of the vocab?
3.    Baseline accuracy
4.   Many models are overfit —> better hyper parameter tuning
5.  Remember DS is an iterative process -> Either mention as Future work, or limitation of the model
Use only upvoted post
Use non-moderator posts - Remove moderated posts
Add to stop words - Hey, hello, May, June etc.
Look at the misclassification post

can use 'pickle' to save model

Google collaborators is a Paas
Runtime -> change runtime type
Tools set to GPU for higher gpu power
None is cpu
 tpu is multi dimensional

Resources in which cloud computer is categorised
Iaas
Paas
Saas = software as a service
Eg email

aws/Sagemaker similar to google cola


VM vs containers

Dockers
Windows home cat run docker. Only window professional.
Mac can run

NIC = network interface card

---------------------------------------------WEEK 7 DAY 2 -------------------------------------------------------------
---------------------------------------------TUESDAY 19 MAY------------------------------------------------------------
------------------------------------------CLUSTERING K-Means & DBSCAN--------------------------------------------------
Container engine is the docker
Container is the app packaged with the dependency

Clustering:

Inertia - measure intra-cluster distance (within cluster) (want this to be small)
Silhouette score - measure inter-cluster distance (between cluster) (want this to be large)(closer to 1 better)

If target value y given but still want use clustering, can use metrics like homogeneity, completeness and v-measure.

Its called classes when we have y. Called clusters when no y.

Unsupervised learning methods:
- Network analysis
- Topic modeling (algorithm: latent dirichlet allocation)
- PCA: dimensionality reduction

clustering algorithms:
* K-Means (mean centroids)
* DBSCAN (density based)
* Hierarchical (nested clusters by merging or splitting successively)
* Affinity Propagation (graph based approach to let points 'vote' on their preferred 'exemplar')
* Mean Shift (can find number of clusters)
* Spectral Clustering
* Agglomerative Clustering (suite of algorithms all based on applying the same criteria/characteristics of one cluster to others)

Methods in unsupervised learning can be applied to supervised learning problems

K-means Clustering cons:
-prone to error for uniform distr
-sensitive to outlier

K-means works well only if each clusters  points are close to its respective centroid

Clustering cannot use to predict any unknown point. Clustering does not do any prediction. It merely group.
Using the cluster output however you can build into classification model to predict. 

ss.inverse_transform : to scale back using fitted mean and sd

DBSCAN - density based spatial clustering
Compensate outlier that kmeans is sensitive to.
It clusters area with high density.

2 main hyperparameter:
- Min_samples: number of points required for a cluster
- Epsilon: the bigger, the further it takes points

Because clustering models are based on distance, we don't want the magnitude of our features to affect the algorithm. Therefore, when clustering you should always scale your data.

df.groupby('cluster').mean().T[[0,1]]
# to look to decide what the clusters imply

---------------------------------------------WEEK 7 DAY 3 -------------------------------------------------------------
-------------------------------------------WEDNESDAY 20 MAY------------------------------------------------------------
----------------------------------------Hierarchical Clustering & PCA--------------------------------------------------
For regression we focus on reducing residual

For classification we focus on reducing misclassification

hierarchical clustering take way longer than k means clustering

Simple linkage use smallest

If too many points, then don’t plot dendogram.
Look at fcluster

Pca compress data in systematic way. Can use as preprocessing 

PCA: good for dimensionality reduction and visualise feature space

Pca is linear transformation of original space

PCA lose interpretation of features unlike linear regression

Pca always choose the top pca component that explains max var in data

If data is very random and no specific direction and quite uniformly distributed, then pca will not be good cause cannot really find good maximum variance. Cause the variance of x will be quite evenly distributed. Eg 25% each for 4 variables. So each variable explain the data equally. 
r^2 is about variance of target y.

Pca is a transformer instead of estimator

Pca creates its pca feature which is a linear combi of original features

Metrics for PCA:
var_exp = pca.explained_variance_ratio_
print('Explained variance:            ', var_exp)
cum_var_exp = np.cumsum(var_exp)
print('Cumulative explained variance: ', cum_var_exp)

PCA is unsupervised learning. No y to do cross_val_score

---------------------------------------------WEEK 7 DAY 4 -------------------------------------------------------------
-------------------------------------------THURSDAY 21 MAY------------------------------------------------------------
-------------------------------------------RECOMMENDER SYSTEM----------------------------------------------------------
commonly modeled techniques:
- content filtering: map users and items to features space
- collaborative filtering

recommender system look at similarity
---------------------------------------------WEEK 7 DAY 5 -------------------------------------------------------------
-------------------------------------------FRIDAY 22 MAY---------------------------------------------------------------
--------------------------------------CORRELATED DATA AND TIME SERIES--------------------------------------------------
omitted variable bias : bias exist because important predictor left out or omitted from our model

dependence:
-temporal dependence is like time series
-spatial dependence

eg of correlated data:
-time series data
-spatial data
-network data: number of network depends on number of connections my network has

time series: rmbr to sort first just in case before doing any shift or rolling etc.

---------------------------------------------WEEK 8 DAY 2 -------------------------------------------------------------
-------------------------------------------TUESDAY 26 MAY---------------------------------------------------------------
-----------------------------------------------ARIMA--------------------------------------------------
lr,decision trees etc assume obs not corr

time series data can use:
- linear model (omitted bias)(correlated data ignored)
- arima model
- exponential smoothing methods
- Recurrent neural networks (rnn)
- etc

arima good for short term forecast. will soon start predicting mean.

autocorrelation = corr of variable with itself
autoregressive = regress variable on itself (regress newer values on older values). 
ar model: explain long term trends in data. gridsearch hyperparam p.
regress future values on past values

moveing average model =! moving average smoothing
moving average model: takes previous error terms as inputs. isnt identical to boosting but similar. 
explains sudden shocks in data. 
hyperparameter q (number of previous errors) to gridsearch.
predicting y.
work in statsmodels cause better. manually gridsearch to find lowest AIC (akaike information criterion)
time series cnt do cv
regress future values on past errors

intergated:
data must be stationary before applying arima
do difference(d) on y variable to ensure stationarity
augmented Dickey-Fuller test is a hypothesis test that tests for stationarity

score using rmse for time series instead of r^2 even though default r^2 given.

pacf to observe p,q,d,S

---------------------------------------------WEEK 8 DAY 3 -------------------------------------------------------------
-------------------------------------------WEDNESDAY 27 MAY-------------------------------------------------------------
------------------------------------------SPATIAL DATA ANALYSIS---------------------------------------------------------
spatiotemporal data: data vary with space or/and time

why is it a problem to use standard method of analysis for spatiotemporal data?
-observation not independent. they are correlated
- omitted variable bias

tobler first law of geography: 'Everything is related to everything else, but near things are more
related than distant things.'

types of spatial data:
- areal process: non-overlapping region eg zipcode
- geostatistical process: continuous
- point pattern data : random points

areal processes:
-define weights. wii is always 0.

to detect whether spatial autocorr exist: moran's I statistic. is a permutation test

make sure use early time for training and later time for testing
make sure use representative locations as test locations (cluster sample)

---------------------------------------------WEEK 8 DAY 4 -------------------------------------------------------------
-------------------------------------------THURSDAY 28 MAY-------------------------------------------------------------
----------------------------------------------NETWORK ANALYSIS---------------------------------------------------------
eulerian path exist if every node have even number of edges or exactly 2 notes have odd number of edges.

types of network:
undirected: connection extends in both directions. eg fb friends
directed: connection only flow in one direction. eg paying utilities transaction,water pipe, follow someone on ig

cyclic: contains at least one cycle. a cycle exist when a node can be connected to itself by traversing at least one edge
acyclic: contains no cycles. eg decision trees (directed acyclic graph)

multigraph: multiple links connecting to same pair of node

degee: number of links of that node. (in degree/out degree)

average degree: measure of connectivity
degree of distribution of network

represent network with an adjacency matrix

take adjacency matrix raise to nth power. can see how many paths of length n exist from node to node

minimum path length: min number of edges required to traverse from node i to node j
diameter: max shortest path length
connected: exist a path from node i to node j
isomorphic: two graphs are isomorphic if we can twist one graph to look like another without cutting or gluing 

difficulty of network analysis:
-storage
-inference
-visaulization

loss function: quantify residual. lower values are better.
eg. gini, mse, logliklehood
R^2 and accuracy are not loss function.

goal of ols is to find beta that minimize mse

gradient descend is iterative
eg of more advanced first order gradient descend: 
- iteratively reweighted least squares (IRLS) (GLM uses this)
-adaptive momentum(adam) (neural network use this)
-rms propagation (RMSprop) (neural network use this)

second order gradient descend: newton method

step size: learning rate

---------------------------------------------WEEK 9 DAY 1 -------------------------------------------------------------
---------------------------------------------MONDAY 1 JUNE-------------------------------------------------------------
-------------------------------------INTRO TO NEURAL NETWORK AND KERAS-------------------------------------------------

NN requires huge computing power that computer in the past cannot, hence lessed used though available. 
NN are statistical models. can be used for both regression and classification.
NN mimic neuron brain process.

our keras is built on to of tensorflow.

convolutional neural network deals with image data.
recurrent neural network deals with sequence data such as time series or natural language data

inverse logit = sigmoid function

input layer -> 2 hidden layer -> output layer = 3 layer model

cons of NN: overfitting

types:
feed forward: calculations are passed down network in one direction
fully connected: each node is conencted to every node in previous layer, and every node in next later

constructing NN need decide:
- no of hidden layer (more tends to mean more complexity of relationship learned)
- no of nodes in each hidden layer
- choice of activation functions: transformation applied to output of layer. 
- loss function

to get best result, need trial and error. no hard rule.

add noise and see how NN perform to see if its good

for explanability: 
- add noise see how weights change
- reference dataset to compare result

activation functions:
-sigmoid activation function. ranges from 0 to 1. preferred for use in output layer
-Rectified linear unit (ReLU) most common activation for hidden later.
never use identity activation as a hidden layer activation. it returns itself.

softmax: extension of sigmoid to multiclass classification
output activation for multiclass classification.
vector of probabilities

hidden layer activation: transformation to allow NN to learn complex relationship

for regression, can just use linear/identity acitivation. no need transform output. 

NN are fitted using gradient descent with respect to loss function.

epoch : one run through full dataset
minibatch: smaller subsampling of full dataset


---------------------------------------------WEEK 9 DAY 2 -------------------------------------------------------------
---------------------------------------------TUESDAY 2 JUNE------------------------------------------------------------
-----------------------------DEEP LEARNING REGLUARIZATION AND WORD VECTOR----------------------------------------------
topology = architecture of network

building nn using tensorflow(by google):
topology: 
-no of input nodes (number of features)
-no of output nodes
-no of hidden layers
- no of nodes each layer

compile network:
-loss function: mse,binary_crossentropy,categorical_crossentropy
-optimizer: adam,rmsprop,agrad
-metrics

fit model:
-batch-size: subset of training data on which the weights get updated
-epoch: the training dataset passed through the NN

each neuron is the linear combination of the features coming to it

training: forward and backward proogation
inference: forward propagation

types of NN:
- fully connected NN
-covolutional NN
-recurrent NN

techinique to prevent overfittig in NN:
- L1 and L2 norm regularization
- dropout
- early stopping

L2 (lasso)(sum of sq of weights) more aggressive than L1 (ridge)(sum of weights).

l1_l2 is elasticnet

dropout regularization: make less dependent on training data. 
dropout only for hidden layer. not for output or input.

dropout possible to do with l1 or l2.

normally ppl just write each dense layer out 

continous bag of words: 2 layer NN.
after training, the hidden layer is the word2vec.

word2vec can also use for sentimental analysis to get the similar words

corpus trained affect the word vector

countvectorizer: vector to word
word2vec: word to vec

LDA (topic modeling) is unsupervised learning. dimensionality reduction technique

---------------------------------------------WEEK 9 DAY 3 --------------------------------------------------------------
---------------------------------------------WEDNESDAY 3 JUNE-----------------------------------------------------------
----------------------------------------CONVOLUTION NEURAL NETWORKS ----------------------------------------------------
applications: neural style transfer, image classification, object detection
no of params if use fully connected NN: area size of image(256*256) * no of hidden notes
RGB image is 3d

vertical edge filter

forward propogation: give output
backward propagation: update weights

to ensure corner pixels get same influence on convul image, perform padding

coonvolution+relu+pooling = feature engineering

filter usually in order of 2. not too big

stride increase to compress image
---------------------------------------------WEEK 9 DAY 4 --------------------------------------------------------------
---------------------------------------------THURSDAY 4 JUNE-----------------------------------------------------------
----------------------------------------RECURRENT NEURAL NETWORKS ----------------------------------------------------
RNN use on sequence data.
eg autocompletion, text to speech, time series

rnn suffer from short term memory due to back propagation though time (bptt).
no problem if use lstm and gru. but cons are slower

types of rnn:
-vanilla nn. (one to one)(fixed input fixed output). eg image classification
-sequence output. (one to many) eg image captioning
-sequence input. (many to one) eg sentiment classification
-sequence input and output. (many to many) eg machine translation
-synced sequence input and output. (many to many) eg video classification on frame level

Long short term memory(LSTM) got forget gate (decide which values of memory cell to reset)(sigmoid layer).
LSTM slightly faster than GRU (Gated Recurrent units) cause GRU combine forget and input into update gate

popular complex architecture: vgg16, inceptionv3

---------------------------------------------WEEK 10 DAY 1 ------------------------------------------------------------------
---------------------------------------------MONDAY 8 JUNE-------------------------------------------------------------------
----------------------------P4 AND MACHINE LEARNING & DEEP LEARNING APPLICATIONS --------------------------------------------
P4 feedback:
plots not make it too complicated. be selective on what you show
label axis

machine learning vs deep learning approach:
feature extraction and classification separate vs together

if data not big, use predefined embedding like glove or word2vec. otherwise can customise

embedding is vector representation of a sentence

if use predefined embedding, may be better off not using lematize for our data

sematic similarity: similarity between 2 sentences

state of art NLP: bert

---------------------------------------------WEEK 10 DAY 2 ------------------------------------------------------------------
---------------------------------------------TUESDAY 9 JUNE-------------------------------------------------------------------
--------------------------------------------------SQL-------------------------------------------------------------------------

For those who are unable to restore database from pgadmin, we can do it via the command line. 
Open you terminal or command prompt:
set "PGPASSWORD=postgres"
/Library/PostgreSQL/12/bin/pg_restore --host "localhost" --port "5432" --username "postgres" --dbname "pagila” --verbose "/Users/Ethan/Desktop/sql/pagila.tar”

Change “ to “ again in terminal

The query to get all tables in a database:
SELECT "table_name","column_name", "data_type", "table_schema"
FROM INFORMATION_SCHEMA.COLUMNS
WHERE "table_schema" = 'public'
ORDER BY table_name

---------------------------------------------WEEK 10 DAY 3 ------------------------------------------------------------------
---------------------------------------------WEDNESDAY 10 JUNE-------------------------------------------------------------------
---------------------------------------------SCALA AND SPARK----------------------------------------------------------------------


---------------------------------------------WEEK 11 DAY 1 ------------------------------------------------------------------
----------------------------------------------MONDAY 15 JUNE-------------------------------------------------------------------
-------------------------------------------BAYES INTRO AND INFERENCE------------------------------------------------------------
bayesian approach: 
-estimate the posterior distribution.
-assume population parameter is distribution
-credible interval: there is 95% chance the true pop mean is between ....
-P(hypo|data)
-post = (likelihood*prior)/normalizing constant
-interest: posterior distribution
-posterior distribtn is more strongly influenced by whichever (prior or likelihood) that has a smaller std deviatn

frequentist approach: 
- long-run behaviour
-assume population parameter is constant
-confidence interval: there is 95% confident the true pop mean is between ....
-p-value = P(data|null hypo) : likelihood

process of inference: population-> sampling-> inference (confidence interval,hypo testing)

conjugacy:

---------------------------------------------WEEK 11 DAY 2 ------------------------------------------------------------------
----------------------------------------------TUESDAY 16 JUNE-------------------------------------------------------------------
------------------------------ PyMC & BAYESIAN REGRESSION, MARKOV CHAIN MONTE CARLO--------------------------------------------



------------------------------------------------LINKS -----------------------------------------------------------------
Run code: htt  \ ps://repl.it/
https://www.hackerrank.com/challenges/nested-list/problem
https://www.python.org/dev/peps/pep-0008/#tabs-or-spaces
https://docs.python.org/3/library/exceptions.html
https://docs.python.org/3/tutorial/errors.html
https://campus.datacamp.com/courses/kaggle-r-tutorial-on-machine-learning/chapter-3-improving-your-predictions-through-random-forests?ex=1
https://automatetheboringstuff.com/
jupyter viewer for github: https://nbviewer.jupyter.org/
https://www.khanacademy.org/computing/computer-programming/sql
https://www.codewars.com/users/Ethkoh/completed_solutions
Tensorflow Playground: https://playground.tensorflow.org/
papers: https://arxiv.org/
https://machinelearninginterview.com/
https://leetcode.com/problemset/database/
--------------------------------------------TO THINK -------------------------------------------------------------
Think what is enumerate again (practice odds_and_even)
Diff between lambda and list comprehension
Can use?  if roll_1 > roll_2 > roll_3
Is control flow same topic as functions
PEP guide -intentation don’t understand “hanging indent should add a a level”
Cannot remove directory/folder
Why 0.2+0.1 != 0.3
What is this??
def find_longest_word(list_of_words):
    return len(max(list_of_words, key=len))
    
to read:
https://jakevdp.github.io/PythonDataScienceHandbook/03.04-missing-values.html
https://www3.ntu.edu.sg/home/ehchua/programming/webprogramming/Python4_DataAnalysis.html
https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html
https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html
https://towardsdatascience.com/why-sample-variance-is-divided-by-n-1-89821b83ef6d
http://www.nohsteachers.info/pcaso/ap_statistics/PDFs/DegreesOfFreedom.pdf
https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-are-degrees-of-freedom-in-statistics
https://www.edureka.co/blog/python-regex/
https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/
a lot link in 3.01 ipy 
https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html
https://towardsdatascience.com/basic-time-series-manipulation-with-pandas-4432afee64ea
https://towardsdatascience.com/how-to-show-all-columns-rows-of-a-pandas-dataframe-c49d4507fcf
https://www.data-blogger.com/2017/11/15/python-matplotlib-pyplot-a-perfect-combination/
https://towardsdatascience.com/the-bias-variance-trade-off-explanation-and-demo-8f462f8d6326
https://readthedocs.org/projects/patsy/downloads/pdf/latest/
https://thispointer.com/python-pandas-how-to-drop-rows-in-dataframe-by-conditions-on-column-values/
bayesian vs frequentist
auc roc
Rmse vs r2 is it always inverse
Find an example with k=no of rows
https://netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf
https://netflixprize.com/assets/GrandPrize2009_BPC_PragmaticTheory.pdf
https://netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf
https://arxiv.org/pdf/1704.03477.pdf
https://www.datacamp.com/community/tutorials/wordcloud-python
https://kavita-ganesan.com/news-classifier-with-logistic-regression-in-python/#.XsHEwNMzYcg
https://medium.com/@annabiancajones/sentiment-analysis-on-reviews-feature-extraction-and-logistic-regression-43a29635cc81
http://rstudio-pubs-static.s3.amazonaws.com/21395_3c4bfb6f49cd4724ba3c6453663adf95.html
https://ieeexplore.ieee.org/document/8781773
---------------------------------------------SELF RESEARCH---------------------------------------------------------
# plot subplots using pandas lib
for idx, feature in enumerate(df.columns[:-1]):
    df.plot(feature, "cnt", ... , ax=axes.flatten()[idx])
    

# plot subplots using pandas lib
df[:10].plot(kind = 'hist',subplots=True, layout = (3,4) ,legend=False,title = ['Ladder',....])

#types of lemmatization using the following python packages
https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#:~:text=2.,and%20most%20commonly%20used%20lemmatizers.
-Wordnet Lemmatizer
-Spacy Lemmatizer
-TextBlob
-CLiPS Pattern
-Stanford CoreNLP
-Gensim Lemmatizer
-TreeTagger
--------------------------------------------GOOD SAMPLE---------------------------------------------------------------
#njob=-1 to use all cores

# trick to download github ipynb
click raw->right click->save link as
#if folder just dl as zip

# Drop the `Unnamed: 0` column.
df.drop('Unnamed: 0', axis=1, inplace=True)

# Drop NAs.
df.dropna(inplace=True)

# Create dummies for the `ChestPain`, `Thal`, and `AHD` columns.
# Be sure to set `drop_first=True`.
df = pd.get_dummies(df,
                    columns=['ChestPain', 'Thal', 'AHD'],
                    drop_first=True)

# Define X and y.
X = df.drop('AHD_Yes', axis='columns')
y = df['AHD_Yes']

# Split data into training and testing sets.
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    random_state=42,
                                                    stratify=y)
                                                    
                                                    
# to 1 and 0                                                     
train['Cabin'] = train['Cabin'].notnull().astype(int)            

SQL
CAST -chop away decimal
ROUND -round but still leaves 0. use cast and round tgt to remove

# wordcloud
# Generate a preliminary word cloud using stopwords in-built in wordcloud
wordcloud = WordCloud(width = 1700, height = 800, background_color="white",stopwords='english').generate(text)

# Display the generated image
plt.figure(figsize = (13, 6))
# interpolation bilinear makes the wordcloud's text smoother easier to read
plt.imshow(wordcloud,interpolation="bilinear")
plt.axis("off")
plt.show()
----------------------------------------------------P3 outline---------------------------------------------------
version 1:
Remove null
Remove duplicate
Reset index
X_casualconv
X_parenting
y_casualconv
y_parenting
USE STOPwORDS to DO PRELIMINARY WORDCLOUD
X = conc
Y = conc
Reset index
Export to csv as X and y	
Choose stopwords
Split to X_train, y_train, X_test, y_test
Word preprocessing to X_test to clean_test_posts and X_train to clean_train_posts
Turn text into features (count vectoriser): maxfeature = 600
fit_transform(clean_train_[posts)
Baseline accuracy
Fit a Naive Bayes model
Fit a Logistic Regression model
Fit Random Forest Classifier
Combine a few models and get best score
Fine tuning Logistic Regression hyperparameters
Fit Final Logistic Model
Conclusion
Limitations and Recommendation
Sources

version 2:
Load Dataset from CSV
Macro Analysis
Data Cleaning
Remove Null values
Remove Duplicates
Choose stopwords library to use
Number of words in total for each subreddit preprocessing
Calculate and Plot Word Frequency
Create final stopwords
Combine into X and y
Split data into training and testing sets
Words Preprocessing
Turn text into features
Baseline accuracy
Fit a Naive Bayes model
Fit a Logistic Regression model
Fit Random Forest Classifier
Combine a few models and get best score
Fine tuning Logistic Regression hyperparameters
Fit Final Logistic Model
Conclusion
Limitations and Recommendation
Sources

